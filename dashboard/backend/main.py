"""
Factory Digital Twin - Visualization API
FastAPI backend serving Neo4j graph data for web dashboard
"""

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from neo4j import GraphDatabase
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
import asyncio
import json
from datetime import datetime, timedelta
import logging
import yaml
import random

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Factory Digital Twin API",
    description="Graph visualization and analytics API",
    version="1.0.0"
)

# CORS middleware for React frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001", "http://localhost:5173"],  # React dev servers
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Neo4j connection
import os
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "factory_twin_2025")

driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))


# Pydantic models
class GraphNode(BaseModel):
    id: str
    name: str
    type: str
    status: Optional[str] = None
    properties: Dict[str, Any] = {}


class GraphLink(BaseModel):
    source: str
    target: str
    type: str
    properties: Dict[str, Any] = {}


class GraphData(BaseModel):
    nodes: List[GraphNode]
    links: List[GraphLink]
    metadata: Dict[str, Any] = {}


class AssetFilter(BaseModel):
    types: Optional[List[str]] = None
    zones: Optional[List[str]] = None
    status: Optional[List[str]] = None
    search: Optional[str] = None


# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
        logger.info(f"WebSocket connected. Total: {len(self.active_connections)}")

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        logger.info(f"WebSocket disconnected. Total: {len(self.active_connections)}")

    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except:
                pass


manager = ConnectionManager()


# API Endpoints

@app.get("/")
def read_root():
    return {
        "name": "Factory Digital Twin API",
        "version": "1.0.0",
        "endpoints": {
            "graph": "/api/graph",
            "assets": "/api/assets",
            "stats": "/api/stats",
            "search": "/api/search/{query}",
            "websocket": "/ws"
        }
    }


@app.get("/api/stats")
def get_statistics():
    """Get overall graph statistics"""
    with driver.session() as session:
        # Get basic counts
        stats_result = session.run("""
            MATCH (a:Asset)
            RETURN
                count(a) as totalAssets,
                collect(DISTINCT a.type) as assetTypes
        """)
        stats_record = stats_result.single()

        # Get online count
        online_result = session.run("""
            MATCH (a:Asset)
            WHERE a.status IN ['running', 'online']
            RETURN count(a) as onlineAssets
        """)
        online_record = online_result.single()

        # Get error count
        error_result = session.run("""
            MATCH (a:Asset)
            WHERE a.status IN ['error', 'offline', 'unreachable']
            RETURN count(a) as errorAssets
        """)
        error_record = error_result.single()

        # Get relationships
        rel_result = session.run("""
            MATCH ()-[r]-()
            RETURN count(r) / 2 as totalRelationships,
                   collect(DISTINCT type(r)) as relationshipTypes
        """)
        rel_record = rel_result.single()

        total_assets = stats_record["totalAssets"]
        online_assets = online_record["onlineAssets"]

        return {
            "totalAssets": total_assets,
            "onlineAssets": online_assets,
            "errorAssets": error_record["errorAssets"],
            "uptimePercent": round(100.0 * online_assets / total_assets, 1) if total_assets > 0 else 0.0,
            "totalRelationships": rel_record["totalRelationships"],
            "assetTypes": stats_record["assetTypes"],
            "relationshipTypes": rel_record["relationshipTypes"]
        }


@app.get("/api/assets/types")
def get_asset_types():
    """Get list of all asset types"""
    with driver.session() as session:
        result = session.run("""
            MATCH (a:Asset)
            RETURN DISTINCT a.type as type, count(*) as count
            ORDER BY count DESC
        """)
        return [{"type": record["type"], "count": record["count"]} for record in result]


@app.get("/api/zones")
def get_zones():
    """Get ISA-95 security zones with health metrics"""
    with driver.session() as session:
        # Get all assets and aggregate by assigned security zone
        result = session.run("""
            MATCH (asset:Asset)
            RETURN asset.type as type,
                   asset.status as status,
                   1 as count
        """)

        # Aggregate assets by zone in Python
        zone_stats = {}
        for record in result:
            asset_type = record["type"]
            status = record["status"]
            zone = get_default_zone(asset_type)

            if zone not in zone_stats:
                zone_stats[zone] = {
                    "id": None,
                    "zone": zone,
                    "level": zone,
                    "security": "Medium",
                    "total": 0,
                    "online": 0,
                    "warning": 0,
                    "offline": 0,
                    "color": "#94a3b8"
                }

            zone_stats[zone]["total"] += 1

            if status in ['running', 'online']:
                zone_stats[zone]["online"] += 1
            elif status in ['warning', 'degraded']:
                zone_stats[zone]["warning"] += 1
            elif status in ['offline', 'error']:
                zone_stats[zone]["offline"] += 1

        # Set colors and security levels
        zone_colors = {
            'Level 0 - Process': ('#ef4444', 'Critical'),
            'Level 1 - Control': ('#f59e0b', 'High'),
            'Level 2 - Supervisory': ('#eab308', 'High'),
            'Level 3 - Operations': ('#22c55e', 'Medium'),
            'Level 4 - Enterprise': ('#3b82f6', 'Low'),
            'Unassigned': ('#94a3b8', 'Medium')
        }

        for zone, stats in zone_stats.items():
            if zone in zone_colors:
                stats["color"] = zone_colors[zone][0]
                stats["security"] = zone_colors[zone][1]

        # Return sorted by zone level
        return sorted(zone_stats.values(), key=lambda x: x["zone"])


@app.post("/api/graph")
def get_graph(filters: AssetFilter = None):
    """Get graph data with optional filters"""
    with driver.session() as session:
        # Build dynamic query based on filters
        where_clauses = []
        params = {}

        if filters:
            if filters.types:
                where_clauses.append("a.type IN $types")
                params["types"] = filters.types

            if filters.status:
                where_clauses.append("a.status IN $status")
                params["status"] = filters.status

            if filters.search:
                where_clauses.append("a.name CONTAINS $search")
                params["search"] = filters.search

        where_clause = " AND ".join(where_clauses) if where_clauses else "true"

        query = f"""
            MATCH (a:Asset)
            WHERE {where_clause}
            OPTIONAL MATCH (a)-[r]->(b:Asset)
            WHERE {where_clause.replace('a.', 'b.')}
            OPTIONAL MATCH (a)-[:LOCATED_IN]->(space:Space)
            OPTIONAL MATCH (a)-[:BELONGS_TO_ZONE]->(zone:Zone)
            RETURN
                collect(DISTINCT {{
                    id: a.id,
                    name: a.name,
                    type: a.type,
                    status: a.status,
                    ipAddress: a.ipAddress,
                    manufacturer: a.manufacturer,
                    currentValue: a.currentValue,
                    unit: a.unit,
                    location: space.name,
                    matterportSpaceId: space.matterportSpaceId,
                    securityZone: zone.isaLevel
                }}) as nodes,
                collect(DISTINCT {{
                    source: a.name,
                    target: b.name,
                    type: type(r),
                    properties: properties(r)
                }}) as links
        """

        result = session.run(query, params)
        record = result.single()

        if not record:
            return {"nodes": [], "links": [], "metadata": {}}

        # Filter out null links
        links = [link for link in record["links"] if link["target"] is not None]

        # Enhance nodes with MCP tools, logging, sub-agents, security zones, and team ownership
        enhanced_nodes = []
        for node in record["nodes"]:
            asset_type = node.get("type", "Unknown")
            capabilities = get_default_capabilities(asset_type)

            enhanced_node = {**node}
            # Ensure each node has a valid ID (use name if id is null)
            if not enhanced_node.get("id"):
                enhanced_node["id"] = enhanced_node.get("name")

            # Assign security zone based on asset type if not present
            if not enhanced_node.get("securityZone"):
                enhanced_node["securityZone"] = get_default_zone(asset_type)

            # Assign team ownership
            zone = enhanced_node.get("securityZone", "Unassigned")
            enhanced_node["teamOwnership"] = get_team_ownership(asset_type, zone)

            enhanced_node["mcpTools"] = node.get("mcpTools", capabilities["mcpTools"])
            enhanced_node["logging"] = node.get("logging", capabilities["logging"])
            enhanced_node["subAgents"] = node.get("subAgents", capabilities["subAgents"])

            enhanced_nodes.append(enhanced_node)

        return {
            "nodes": enhanced_nodes,
            "links": links,
            "metadata": {
                "nodeCount": len(enhanced_nodes),
                "linkCount": len(links),
                "timestamp": datetime.utcnow().isoformat()
            }
        }


@app.post("/api/graph/manufacturing")
def get_manufacturing_graph(filters: AssetFilter = None):
    """Get manufacturing-specific subgraph"""
    with driver.session() as session:
        result = session.run("""
            MATCH (a:Asset)
            WHERE a.type IN ['PLC', 'Sensor', 'Robot', 'IndustrialRobot', 'Cobot', 'Conveyor', 'HMI']
            OPTIONAL MATCH (a)-[r]->(b:Asset)
            WHERE b.type IN ['PLC', 'Sensor', 'Robot', 'IndustrialRobot', 'Cobot', 'Conveyor', 'HMI', 'UPS', 'EdgeGateway']
            RETURN
                collect(DISTINCT {
                    id: a.id,
                    name: a.name,
                    type: a.type,
                    status: a.status,
                    currentValue: a.currentValue,
                    unit: a.unit,
                    opcuaEndpoint: a.opcuaEndpoint
                }) as nodes,
                collect(DISTINCT {
                    source: a.id,
                    target: b.id,
                    type: type(r)
                }) as links
        """)

        record = result.single()
        if not record:
            return {"nodes": [], "links": []}

        links = [link for link in record["links"] if link["target"] is not None]
        return {"nodes": record["nodes"], "links": links}


@app.post("/api/graph/network")
def get_network_graph(filters: AssetFilter = None):
    """Get network topology subgraph"""
    with driver.session() as session:
        result = session.run("""
            MATCH (a:Asset)
            WHERE a.type IN ['NetworkSwitch', 'Router', 'Firewall', 'Server']
            OPTIONAL MATCH (a)-[r:CONNECTS_TO]->(b:Asset)
            RETURN
                collect(DISTINCT {
                    id: a.id,
                    name: a.name,
                    type: a.type,
                    status: a.status,
                    ipAddress: a.ipAddress
                }) as nodes,
                collect(DISTINCT {
                    source: a.id,
                    target: b.id,
                    type: type(r)
                }) as links
        """)

        record = result.single()
        if not record:
            return {"nodes": [], "links": []}

        links = [link for link in record["links"] if link["target"] is not None]
        return {"nodes": record["nodes"], "links": links}


@app.post("/api/graph/infrastructure")
def get_infrastructure_graph(filters: AssetFilter = None):
    """Get Nutanix/K8s infrastructure subgraph"""
    with driver.session() as session:
        result = session.run("""
            MATCH (a:Asset)
            WHERE a.type IN ['HyperconvergedCluster', 'Server', 'Storage',
                            'KubernetesCluster', 'KubernetesDeployment', 'UPS']
            OPTIONAL MATCH (a)-[r]->(b:Asset)
            RETURN
                collect(DISTINCT {
                    id: a.id,
                    name: a.name,
                    type: a.type,
                    status: a.status,
                    replicas: a.replicas,
                    namespace: a.namespace
                }) as nodes,
                collect(DISTINCT {
                    source: a.id,
                    target: b.id,
                    type: type(r)
                }) as links
        """)

        record = result.single()
        if not record:
            return {"nodes": [], "links": []}

        links = [link for link in record["links"] if link["target"] is not None]
        return {"nodes": record["nodes"], "links": links}


def get_default_zone(asset_type: str) -> str:
    """Assign ISA-95 security zone based on asset type"""
    # Level 0 - Process (Field devices, sensors, actuators)
    if asset_type in ["Sensor", "IoTDevice", "Camera", "RFID"]:
        return "Level 0 - Process"

    # Level 1 - Control (PLCs, Controllers, Industrial robots)
    if asset_type in ["PLC", "Robot", "IndustrialRobot", "Cobot", "CNC", "HMI"]:
        return "Level 1 - Control"

    # Level 2 - Supervisory (SCADA, HMI servers, local databases)
    if asset_type in ["Gateway", "EdgeGateway", "NetworkSwitch"]:
        return "Level 2 - Supervisory"

    # Level 3 - Operations (MES, manufacturing execution systems)
    if asset_type in ["Server", "Storage", "KafkaBroker", "PostgreSQL", "TimescaleDB"]:
        return "Level 3 - Operations"

    # Level 4 - Enterprise (ERP, business systems)
    if asset_type in ["KubernetesCluster", "Router", "Firewall", "UPS", "BMS"]:
        return "Level 4 - Enterprise"

    return "Unassigned"


def get_team_ownership(asset_type: str, zone: str) -> dict:
    """Assign team ownership based on asset type and ISA-95 zone"""

    # Field Devices & Sensors Team
    if asset_type in ["Sensor", "IoTDevice", "Camera", "RFID"]:
        return {
            "team": "IoT & Sensor Operations",
            "lead": "Sarah Chen",
            "contact": "sarah.chen@factory.com",
            "slack": "#iot-operations",
            "oncall": "+1-555-0101"
        }

    # Automation & Robotics Team
    if asset_type in ["PLC", "Robot", "IndustrialRobot", "Cobot", "CNC"]:
        return {
            "team": "Automation & Robotics",
            "lead": "Michael Torres",
            "contact": "michael.torres@factory.com",
            "slack": "#automation-team",
            "oncall": "+1-555-0102"
        }

    # Control Systems Team
    if asset_type in ["HMI", "SCADA"]:
        return {
            "team": "Control Systems Engineering",
            "lead": "Jennifer Park",
            "contact": "jennifer.park@factory.com",
            "slack": "#control-systems",
            "oncall": "+1-555-0103"
        }

    # Network Infrastructure Team
    if asset_type in ["Gateway", "EdgeGateway", "NetworkSwitch", "Router", "Firewall"]:
        return {
            "team": "Network Infrastructure",
            "lead": "David Kim",
            "contact": "david.kim@factory.com",
            "slack": "#network-ops",
            "oncall": "+1-555-0104"
        }

    # Data Platform Team
    if asset_type in ["Server", "Storage", "KafkaBroker", "PostgreSQL", "TimescaleDB"]:
        return {
            "team": "Data Platform Engineering",
            "lead": "Aisha Patel",
            "contact": "aisha.patel@factory.com",
            "slack": "#data-platform",
            "oncall": "+1-555-0105"
        }

    # Cloud & Enterprise Systems Team
    if asset_type in ["KubernetesCluster", "UPS", "BMS"]:
        return {
            "team": "Cloud & Enterprise Systems",
            "lead": "Robert Martinez",
            "contact": "robert.martinez@factory.com",
            "slack": "#cloud-ops",
            "oncall": "+1-555-0106"
        }

    # Default - General Maintenance Team
    return {
        "team": "General Maintenance & Operations",
        "lead": "Operations Manager",
        "contact": "ops-manager@factory.com",
        "slack": "#general-ops",
        "oncall": "+1-555-0100"
    }


def get_default_capabilities(asset_type: str) -> dict:
    """Get default MCP tools, logging, and sub-agents based on asset type"""
    capabilities = {
        "mcpTools": ["neo4j-query", "asset-inspector"],
        "logging": ["syslog", "event-log"],
        "subAgents": ["diagnostic-agent", "correlation-agent"]
    }

    # Type-specific capabilities
    if asset_type in ["PLC", "Robot", "CNC"]:
        capabilities["mcpTools"].extend(["plc-monitor", "performance-analyzer"])
        capabilities["logging"].extend(["control-log", "production-log"])
        capabilities["subAgents"].append("production-optimizer-agent")
    elif asset_type in ["NetworkSwitch", "Router", "Firewall"]:
        capabilities["mcpTools"].extend(["network-analyzer", "packet-inspector"])
        capabilities["logging"].extend(["netflow", "snmp-log"])
        capabilities["subAgents"].extend(["network-security-agent", "traffic-analyzer-agent"])
    elif asset_type in ["UPS", "PDU", "Generator"]:
        capabilities["mcpTools"].extend(["power-monitor", "energy-analyzer"])
        capabilities["logging"].extend(["power-event-log", "battery-log"])
        capabilities["subAgents"].append("power-management-agent")
    elif asset_type in ["Server", "Database", "Cloud"]:
        capabilities["mcpTools"].extend(["log-analyzer", "performance-monitor"])
        capabilities["logging"].extend(["application-log", "error-log", "access-log"])
        capabilities["subAgents"].extend(["performance-tuning-agent", "security-audit-agent"])
    elif asset_type in ["Sensor", "IoT", "Camera"]:
        capabilities["mcpTools"].extend(["iot-connector", "data-stream-analyzer"])
        capabilities["logging"].extend(["sensor-data-log", "telemetry-log"])
        capabilities["subAgents"].append("iot-analytics-agent")

    return capabilities


@app.get("/api/asset/{asset_id}")
def get_asset_details(asset_id: str):
    """Get detailed information about a specific asset"""
    with driver.session() as session:
        # First try to find by id, if not found try by name
        result = session.run("""
            MATCH (a:Asset)
            WHERE a.id = $assetId OR a.name = $assetId
            OPTIONAL MATCH (a)-[r_out]->(connected_out:Asset)
            OPTIONAL MATCH (a)<-[r_in]-(connected_in:Asset)
            OPTIONAL MATCH (a)-[:LOCATED_IN]->(space:Space)
            OPTIONAL MATCH (a)-[:BELONGS_TO_ZONE]->(zone:Zone)
            RETURN
                properties(a) as properties,
                collect(DISTINCT {type: type(r_out), target: connected_out.name, targetType: connected_out.type}) as outgoing,
                collect(DISTINCT {type: type(r_in), source: connected_in.name, sourceType: connected_in.type}) as incoming,
                space.name as location,
                space.matterportUrl as matterportUrl,
                zone.name as zoneName,
                zone.isaLevel as isaLevel
        """, assetId=asset_id)

        record = result.single()
        if not record:
            raise HTTPException(status_code=404, detail="Asset not found")

        props = record["properties"]
        asset_type = props.get("type", "Unknown")

        # Get capabilities from properties or use defaults
        capabilities = get_default_capabilities(asset_type)
        mcp_tools = props.get("mcpTools", capabilities["mcpTools"])
        logging = props.get("logging", capabilities["logging"])
        sub_agents = props.get("subAgents", capabilities["subAgents"])

        # Get team ownership
        zone = record["zoneName"] or get_default_zone(asset_type)
        team_ownership = get_team_ownership(asset_type, zone)

        return {
            "name": props.get("name"),
            "type": asset_type,
            "status": props.get("status"),
            "properties": props,
            "relationships": {
                "outgoing": [r for r in record["outgoing"] if r["target"]],
                "incoming": [r for r in record["incoming"] if r["source"]]
            },
            "location": {
                "space": record["location"],
                "matterportUrl": record["matterportUrl"]
            },
            "zone": {
                "name": record["zoneName"],
                "level": record["isaLevel"]
            },
            "mcpTools": mcp_tools,
            "logging": logging,
            "subAgents": sub_agents,
            "teamOwnership": team_ownership
        }


@app.get("/api/search/{query}")
def search_assets(query: str):
    """Search assets by name or type"""
    with driver.session() as session:
        result = session.run("""
            MATCH (a:Asset)
            WHERE toLower(a.name) CONTAINS toLower($query)
               OR toLower(a.type) CONTAINS toLower($query)
            RETURN a.id as id, a.name as name, a.type as type, a.status as status
            LIMIT 20
        """, query=query)

        return [dict(record) for record in result]


@app.get("/api/mcp-tools")
def get_mcp_tools():
    """Get all MCP tools and their capabilities"""
    with driver.session() as session:
        result = session.run("""
            MATCH (mcp:MCPServer)-[:PROVIDES]->(tool:MCPTool)
            OPTIONAL MATCH (tool)-[:CAN_EXECUTE]->(asset:Asset)
            WITH mcp, tool, count(DISTINCT asset) as assetCount
            RETURN
                mcp.name as server,
                collect(DISTINCT {
                    id: tool.id,
                    name: tool.name,
                    capability: tool.capability,
                    riskLevel: tool.riskLevel,
                    traversalStrategy: tool.traversalStrategy,
                    assetCount: assetCount
                }) as tools
            ORDER BY mcp.name
        """)

        return [dict(record) for record in result]


@app.post("/api/mcp-tools/{tool_id}/execute")
def execute_mcp_tool(tool_id: str, target_asset_id: str):
    """Simulate MCP tool execution (demo mode)"""
    # In production, this would actually execute the tool
    # For now, return a simulation

    with driver.session() as session:
        # Verify tool can execute on this asset
        result = session.run("""
            MATCH (tool:MCPTool {id: $toolId})
            MATCH (asset:Asset {id: $assetId})
            MATCH (tool)-[:CAN_EXECUTE]->(asset)
            RETURN tool.name as toolName, tool.riskLevel as riskLevel,
                   asset.name as assetName
        """, toolId=tool_id, assetId=target_asset_id)

        record = result.single()
        if not record:
            raise HTTPException(status_code=400, detail="Tool cannot execute on this asset")

        return {
            "status": "simulated",
            "tool": record["toolName"],
            "target": record["assetName"],
            "riskLevel": record["riskLevel"],
            "message": f"Simulated execution of {record['toolName']} on {record['assetName']}",
            "timestamp": datetime.utcnow().isoformat(),
            "requiresApproval": record["riskLevel"] in ["HIGH", "CRITICAL"]
        }


@app.get("/api/data-pipeline")
def get_data_pipeline():
    """Get end-to-end data pipeline visualization"""
    with driver.session() as session:
        result = session.run("""
            MATCH path = (sensor:Asset {type: 'Sensor'})-[:REPORTS_TO]->
                         (plc:Asset)-[:CONNECTS_TO]->
                         (gateway:Asset)-[:PUBLISHES_TO]->
                         (mqtt:Asset)<-[:DEPENDS_ON]-
                         (historian:Asset)-[:USES_STORAGE]->(storage:Asset)
            RETURN
                [node in nodes(path) | {
                    id: node.id,
                    name: node.name,
                    type: node.type,
                    currentValue: node.currentValue,
                    unit: node.unit
                }] as pipeline,
                length(path) as hops
            LIMIT 5
        """)

        return [{"pipeline": record["pipeline"], "hops": record["hops"]} for record in result]


# WebSocket endpoint for real-time updates
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            # Wait for messages from client (ping/pong)
            data = await websocket.receive_text()

            if data == "ping":
                await websocket.send_json({"type": "pong", "timestamp": datetime.utcnow().isoformat()})

            # In production, this would stream real-time updates from Neo4j
            # For now, send periodic stats updates
            await asyncio.sleep(5)

            # Get current stats
            with driver.session() as session:
                result = session.run("""
                    MATCH (a:Asset {status: 'running'})
                    WITH count(a) as online
                    MATCH (all:Asset)
                    RETURN online, count(all) as total
                """)
                record = result.single()

                await websocket.send_json({
                    "type": "stats_update",
                    "data": {
                        "online": record["online"],
                        "total": record["total"],
                        "timestamp": datetime.utcnow().isoformat()
                    }
                })

    except WebSocketDisconnect:
        manager.disconnect(websocket)


class AIQuery(BaseModel):
    query: str


@app.post("/api/ai/query")
def ai_query(query: AIQuery):
    """AI-powered query processing with natural language understanding"""
    query_lower = query.query.lower()

    with driver.session() as session:
        # Advanced RCA: Root cause for specific asset
        if any(keyword in query_lower for keyword in ['why is', 'root cause of', 'why did', 'what caused']) and any(word in query_lower for word in ['fail', 'offline', 'down']):
            # Extract asset name from query (simple extraction)
            words = query.query.split()
            asset_name = None
            for i, word in enumerate(words):
                if word.lower() in ['is', 'did'] and i + 1 < len(words):
                    asset_name = words[i + 1].strip('?.,')
                    break

            if asset_name:
                # Call the RCA root cause endpoint
                rca_result = find_root_cause({"assetName": asset_name})

                if rca_result and rca_result.get("rootCause"):
                    response = f"ðŸ” **Root Cause Analysis for {asset_name}**\n\n"
                    response += f"**Root Cause:** {rca_result['rootCause']} ({rca_result.get('rootCauseType', 'Unknown')})\n"
                    response += f"**Status:** {rca_result.get('rootCauseStatus', 'Unknown')}\n"

                    if rca_result.get('failureDepth', 0) > 0:
                        response += f"**Failure Depth:** {rca_result['failureDepth']} hops upstream\n\n"

                        if rca_result.get('failureChain'):
                            response += "**Failure Chain:**\n"
                            for i, node in enumerate(rca_result['failureChain'], 1):
                                response += f"{i}. {node['name']} ({node['type']}) - {node['status']}\n"

                    response += f"\n**Analysis:** {rca_result.get('analysis', 'No analysis available')}"

                    return {
                        "response": response,
                        "queryType": "rca_advanced",
                        "data": rca_result
                    }

        # Cascade impact analysis
        elif any(keyword in query_lower for keyword in ['what if', 'impact of', 'affect', 'cascade', 'blast radius']):
            words = query.query.split()
            asset_name = None

            # Try to extract asset name
            for i, word in enumerate(words):
                if word.lower() in ['if', 'of'] and i + 1 < len(words):
                    asset_name = words[i + 1].strip('?., ')
                    break

            if asset_name:
                cascade_result = analyze_cascade_impact({"assetName": asset_name})

                if cascade_result and not cascade_result.get("error"):
                    response = f"ðŸ’¥ **Cascade Impact Analysis for {asset_name}**\n\n"
                    response += f"**Severity:** {cascade_result.get('severity', 'unknown').upper()}\n"
                    response += f"**Currently Affected:** {cascade_result.get('currentlyAffected', 0)} assets\n"
                    response += f"**Total Downstream:** {cascade_result.get('totalDownstream', 0)} assets\n"
                    response += f"**Impact Radius:** {cascade_result.get('impactRadius', 0)} hops\n\n"
                    response += f"**Analysis:** {cascade_result.get('analysis', '')}\n\n"

                    if cascade_result.get('affectedAssets'):
                        critical_assets = [a for a in cascade_result['affectedAssets'] if a.get('isAffected')]
                        if critical_assets:
                            response += f"**Critical Affected Assets:**\n"
                            for asset in critical_assets[:5]:
                                response += f"â€¢ {asset['name']} ({asset['type']}) - {asset['status']}\n"

                    return {
                        "response": response,
                        "queryType": "cascade_impact",
                        "data": cascade_result
                    }

        # Network issues
        elif any(keyword in query_lower for keyword in ['network', 'connectivity', 'isolated', 'unreachable', 'switch', 'router']):
            network_issues = analyze_network_path_failures()

            if network_issues:
                response = f"ðŸŒ **Network Path Failure Analysis**\n\n"
                response += f"Found {len(network_issues)} network issues:\n\n"

                for i, issue in enumerate(network_issues[:3], 1):
                    response += f"**{i}. {issue['failedNetworkDevice']}** ({issue['deviceType']})\n"
                    response += f"   Severity: {issue['severity'].upper()}\n"
                    response += f"   Isolated Devices: {issue['isolatedCount']}\n"
                    response += f"   Recommendation: {issue['recommendation']}\n\n"

                return {
                    "response": response,
                    "queryType": "network_failure",
                    "data": {"issues": network_issues}
                }

        # Power issues
        elif any(keyword in query_lower for keyword in ['power', 'ups', 'battery', 'electrical']):
            power_issues = analyze_power_disruptions()

            if power_issues:
                response = f"âš¡ **Power Disruption Analysis**\n\n"
                response += f"Found {len(power_issues)} power issues:\n\n"

                for i, issue in enumerate(power_issues[:3], 1):
                    response += f"**{i}. {issue['powerSource']}** ({issue['sourceType']})\n"
                    response += f"   Severity: {issue['severity'].upper()}\n"
                    response += f"   Critical Equipment: {issue['criticalEquipment']}\n"
                    response += f"   Total Affected: {issue['affectedCount']}\n"
                    response += f"   Risk Score: {issue['riskScore']}\n"
                    response += f"   âš ï¸ {issue['recommendation']}\n\n"

                return {
                    "response": response,
                    "queryType": "power_disruption",
                    "data": {"issues": power_issues}
                }

        # Performance/bottleneck issues
        elif any(keyword in query_lower for keyword in ['bottleneck', 'performance', 'slow', 'degraded', 'lag']):
            perf_issues = analyze_performance_degradation()

            if perf_issues:
                response = f"ðŸ“ˆ **Performance Degradation Analysis**\n\n"
                response += f"Found {len(perf_issues)} performance issues:\n\n"

                for i, issue in enumerate(perf_issues[:3], 1):
                    response += f"**{i}. {issue['asset']}** ({issue['type']})\n"
                    response += f"   Severity: {issue['severity'].upper()}\n"
                    response += f"   Bottleneck Score: {issue['bottleneckScore']}\n"
                    response += f"   Pattern: {issue['pattern']}\n"
                    response += f"   ðŸ’¡ {issue['recommendation']}\n\n"

                return {
                    "response": response,
                    "queryType": "performance_degradation",
                    "data": {"issues": perf_issues}
                }

        # Basic RCA: List all offline/error assets
        elif any(keyword in query_lower for keyword in ['rca', 'root cause', 'offline', 'down', 'failed', 'failure']):
            result = session.run("""
                MATCH (problem:Asset)
                WHERE problem.status IN ['offline', 'error', 'unreachable']
                OPTIONAL MATCH path = (problem)<-[:DEPENDS_ON|POWERED_BY|CONNECTS_TO*1..3]-(cause:Asset)
                WHERE cause.status IN ['offline', 'error', 'unreachable']
                WITH problem,
                     collect(DISTINCT cause) as potentialCauses,
                     collect(DISTINCT path) as paths
                RETURN {
                    asset: properties(problem),
                    causes: [c in potentialCauses | properties(c)],
                    pathCount: size(paths)
                } as analysis
                LIMIT 10
            """)

            analyses = [dict(record["analysis"]) for record in result]

            if not analyses:
                return {
                    "response": "âœ… Great news! No offline or failed assets detected. All systems are operational.",
                    "queryType": "rca",
                    "data": {"assets": []}
                }

            response = f"ðŸ” Root Cause Analysis Results:\n\n"
            response += f"Found {len(analyses)} assets with issues:\n\n"

            for i, analysis in enumerate(analyses[:5], 1):
                asset = analysis['asset']
                causes = analysis.get('causes', [])
                response += f"{i}. **{asset['name']}** ({asset['type']})\n"
                response += f"   Status: {asset.get('status', 'unknown')}\n"

                if causes:
                    response += f"   Potential root causes ({len(causes)}):\n"
                    for cause in causes[:3]:
                        response += f"   â€¢ {cause['name']} ({cause['type']}) - {cause.get('status', 'unknown')}\n"
                else:
                    response += f"   No upstream dependencies found\n"
                response += "\n"

            if len(analyses) > 5:
                response += f"... and {len(analyses) - 5} more\n"

            return {
                "response": response,
                "queryType": "rca",
                "data": {"assets": [a['asset'] for a in analyses]}
            }

        # Performance analysis
        elif any(keyword in query_lower for keyword in ['performance', 'slow', 'degraded', 'warning']):
            result = session.run("""
                MATCH (a:Asset)
                WHERE a.status IN ['warning', 'degraded']
                   OR (a.currentValue IS NOT NULL AND a.currentValue > 90)
                RETURN {
                    name: a.name,
                    type: a.type,
                    status: a.status,
                    currentValue: a.currentValue,
                    unit: a.unit
                } as asset
                LIMIT 10
            """)

            assets = [dict(record["asset"]) for record in result]

            if not assets:
                return {
                    "response": "âœ… All systems are performing within normal parameters.",
                    "queryType": "performance",
                    "data": {"assets": []}
                }

            response = f"âš¡ Performance Analysis:\n\n"
            response += f"Found {len(assets)} assets with performance concerns:\n\n"

            for i, asset in enumerate(assets, 1):
                response += f"{i}. {asset['name']} ({asset['type']})\n"
                if asset.get('status'):
                    response += f"   Status: {asset['status']}\n"
                if asset.get('currentValue'):
                    response += f"   Value: {asset['currentValue']}{asset.get('unit', '')}\n"
                response += "\n"

            return {
                "response": response,
                "queryType": "performance",
                "data": {"assets": assets}
            }

        # Critical assets check
        elif any(keyword in query_lower for keyword in ['critical', 'important', 'high priority', 'alert']):
            result = session.run("""
                MATCH (a:Asset)
                WHERE a.status = 'error' OR a.type IN ['PLC', 'Server', 'KubernetesCluster']
                OPTIONAL MATCH (a)-[:CONTROLS|MANAGES]->(dependent:Asset)
                WITH a, count(DISTINCT dependent) as dependentCount
                RETURN {
                    name: a.name,
                    type: a.type,
                    status: a.status,
                    dependentCount: dependentCount
                } as asset
                ORDER BY dependentCount DESC
                LIMIT 10
            """)

            assets = [dict(record["asset"]) for record in result]

            response = f"ðŸš¨ Critical Assets Status:\n\n"

            for i, asset in enumerate(assets, 1):
                response += f"{i}. {asset['name']} ({asset['type']})\n"
                response += f"   Status: {asset.get('status', 'unknown')}\n"
                response += f"   Controls {asset['dependentCount']} assets\n\n"

            return {
                "response": response,
                "queryType": "critical",
                "data": {"assets": assets}
            }

        # Network health check
        elif any(keyword in query_lower for keyword in ['network', 'connectivity', 'connection', 'switch', 'router']):
            result = session.run("""
                MATCH (net:Asset)
                WHERE net.type IN ['NetworkSwitch', 'Router', 'Firewall']
                OPTIONAL MATCH (net)-[r:CONNECTS_TO]->()
                WITH net, count(r) as connections
                RETURN {
                    name: net.name,
                    type: net.type,
                    status: net.status,
                    ipAddress: net.ipAddress,
                    connections: connections
                } as asset
            """)

            assets = [dict(record["asset"]) for record in result]

            online_count = sum(1 for a in assets if a.get('status') in ['running', 'online'])

            response = f"ðŸŒ Network Health Report:\n\n"
            response += f"Network devices: {len(assets)} ({online_count} online)\n\n"

            for i, asset in enumerate(assets, 1):
                status_icon = "âœ…" if asset.get('status') in ['running', 'online'] else "âŒ"
                response += f"{status_icon} {asset['name']} ({asset['type']})\n"
                if asset.get('ipAddress'):
                    response += f"   IP: {asset['ipAddress']}\n"
                response += f"   Connections: {asset['connections']}\n\n"

            return {
                "response": response,
                "queryType": "network",
                "data": {"assets": assets}
            }

        # Default: Asset search
        else:
            # Extract potential asset name from query
            result = session.run("""
                MATCH (a:Asset)
                WHERE toLower(a.name) CONTAINS toLower($query)
                   OR toLower(a.type) CONTAINS toLower($query)
                RETURN {
                    name: a.name,
                    type: a.type,
                    status: a.status,
                    ipAddress: a.ipAddress
                } as asset
                LIMIT 10
            """, query=query.query)

            assets = [dict(record["asset"]) for record in result]

            if not assets:
                return {
                    "response": f"I couldn't find any assets matching '{query.query}'.\n\nTry asking:\nâ€¢ 'What's offline?'\nâ€¢ 'Check network health'\nâ€¢ 'Show performance issues'\nâ€¢ 'Find root cause'",
                    "queryType": "search",
                    "data": {"assets": []}
                }

            response = f"Found {len(assets)} assets:\n\n"
            for i, asset in enumerate(assets, 1):
                status_icon = "âœ…" if asset.get('status') in ['running', 'online'] else "âš ï¸" if asset.get('status') in ['warning', 'degraded'] else "âŒ"
                response += f"{i}. {status_icon} {asset['name']} ({asset['type']})\n"
                if asset.get('status'):
                    response += f"   Status: {asset['status']}\n"
                if asset.get('ipAddress'):
                    response += f"   IP: {asset['ipAddress']}\n"
                response += "\n"

            return {
                "response": response,
                "queryType": "search",
                "data": {"assets": assets}
            }


@app.get("/api/executive/issues")
def get_executive_issues():
    """Get critical issues for executive dashboard"""
    with driver.session() as session:
        result = session.run("""
            MATCH (a:Asset)
            WHERE a.status IN ['offline', 'error', 'warning', 'degraded']
            OPTIONAL MATCH (a)-[:LOCATED_IN]->(space:Space)
            RETURN {
                asset: a.name,
                type: a.type,
                status: a.status,
                issue: coalesce(a.failureReason, a.alertReason, a.issue, 'Status: ' + a.status),
                since: coalesce(
                    toString(a.lastFailure),
                    toString(a.lastAlert),
                    toString(a.failureTime),
                    'Recently'
                ),
                severity: CASE
                    WHEN a.status IN ['offline', 'error'] THEN 'critical'
                    WHEN a.status = 'warning' THEN 'high'
                    ELSE 'medium'
                END,
                location: space.name
            } as issue
            ORDER BY
                CASE issue.severity
                    WHEN 'critical' THEN 1
                    WHEN 'high' THEN 2
                    ELSE 3
                END
            LIMIT 10
        """)

        return [dict(record["issue"]) for record in result]


@app.get("/api/executive/performance")
def get_performance_metrics():
    """Get performance and OEE metrics for executive dashboard with historical data"""
    import random
    from datetime import datetime, timedelta

    with driver.session() as session:
        # Get manufacturing equipment performance
        result = session.run("""
            MATCH (a:Asset)
            WHERE a.type IN ['PLC', 'IndustrialRobot', 'Robot', 'Conveyor', 'Machine']
            WITH count(a) as total,
                 sum(CASE WHEN a.status IN ['running', 'online'] THEN 1 ELSE 0 END) as running,
                 sum(CASE WHEN a.status IN ['warning', 'degraded'] THEN 1 ELSE 0 END) as degraded,
                 sum(CASE WHEN a.status IN ['offline', 'error'] THEN 1 ELSE 0 END) as failed
            WITH total, running, degraded, failed,
                 // OEE calculation: (Running / Total) * 100
                 // Adjust for degraded performance (count as 0.7)
                 CASE WHEN total > 0
                      THEN toFloat(running + (degraded * 0.7)) / total * 100
                      ELSE 0
                 END as oeeScore
            RETURN {
                totalEquipment: total,
                running: running,
                degraded: degraded,
                failed: failed,
                oeeScore: round(oeeScore * 10.0) / 10.0,
                performancePercent: round(oeeScore * 10.0) / 10.0
            } as metrics
        """)

        record = result.single()
        if record:
            metrics = dict(record["metrics"])
        else:
            metrics = {
                "totalEquipment": 0,
                "running": 0,
                "degraded": 0,
                "failed": 0,
                "oeeScore": 0.0,
                "performancePercent": 0.0
            }

        # Generate factory-wide performance historical data for the last 10 time periods
        now = datetime.now()
        history = []
        base_oee = metrics.get("oeeScore", 75.0)
        base_health = metrics.get("performancePercent", 80.0)

        # Calculate uptime percentage from current stats
        total_assets = metrics.get("totalEquipment", 1)
        running_assets = metrics.get("running", 0) + metrics.get("degraded", 0)
        base_uptime = (running_assets / total_assets * 100) if total_assets > 0 else 0

        for i in range(10):
            timestamp = now - timedelta(minutes=10-i)
            # Add realistic variation to factory-wide metrics
            oee = max(0, min(100, base_oee + random.uniform(-8, 8)))
            health = max(0, min(100, base_health + random.uniform(-6, 6)))
            uptime = max(0, min(100, base_uptime + random.uniform(-4, 4)))

            history.append({
                "timestamp": timestamp.isoformat(),
                "oee": round(oee, 1),  # Overall Equipment Effectiveness
                "health": round(health, 1),  # Asset Health Score
                "uptime": round(uptime, 1)  # Factory Uptime
            })

        metrics["history"] = history
        return metrics


@app.get("/api/executive/network-health")
def get_network_health():
    """Get network infrastructure health metrics"""
    with driver.session() as session:
        result = session.run("""
            MATCH (a:Asset)
            WHERE a.type IN ['NetworkSwitch', 'Router', 'Firewall', 'AccessPoint']
            WITH count(a) as total,
                 sum(CASE WHEN a.status IN ['running', 'online'] THEN 1 ELSE 0 END) as online,
                 sum(CASE WHEN a.status IN ['warning', 'degraded'] THEN 1 ELSE 0 END) as degraded,
                 sum(CASE WHEN a.status IN ['offline', 'error'] THEN 1 ELSE 0 END) as offline
            WITH total, online, degraded, offline,
                 CASE WHEN total > 0
                      THEN toFloat(online) / total * 100
                      ELSE 0
                 END as healthPercent
            RETURN {
                totalDevices: total,
                online: online,
                degraded: degraded,
                offline: offline,
                healthPercent: round(healthPercent * 10.0) / 10.0
            } as health
        """)

        record = result.single()
        if record:
            return dict(record["health"])
        return {
            "totalDevices": 0,
            "online": 0,
            "degraded": 0,
            "offline": 0,
            "healthPercent": 0.0
        }


def generate_detailed_rca_analysis(rca_data: dict) -> dict:
    """Generate detailed RCA analysis with thought process and reasoning"""
    root_cause = rca_data.get("rootCause", "Unknown")
    root_type = rca_data.get("rootCauseType", "Unknown")
    root_reason = rca_data.get("rootCauseReason", "Unknown failure reason")
    target = rca_data.get("targetAsset", "Unknown")
    depth = rca_data.get("failureDepth", 0)
    chain = rca_data.get("failureChain", [])
    rel_types = rca_data.get("relationshipTypes", [])

    # Build detailed analysis
    analysis = {
        "thoughtProcess": [
            f"1. INITIAL OBSERVATION: Asset '{target}' is experiencing issues and requires root cause investigation.",
            f"2. DEPENDENCY ANALYSIS: Examining upstream dependencies through {len(chain)} assets in the failure chain.",
            f"3. RELATIONSHIP MAPPING: Identified {len(rel_types)} dependency relationships: {', '.join(set(rel_types)) if rel_types else 'none'}.",
            f"4. FAILURE PROPAGATION: Traced {depth} levels deep through the dependency graph.",
            f"5. ROOT IDENTIFICATION: Found '{root_cause}' ({root_type}) as the originating failure point."
        ],
        "evidenceExamined": [
            {
                "category": "Asset Status",
                "finding": f"{root_cause} has status indicating failure",
                "details": f"Failure reason: {root_reason}"
            },
            {
                "category": "Dependency Chain",
                "finding": f"Identified {len(chain)} assets in failure propagation path",
                "details": " â†’ ".join([f"{asset.get('name', 'Unknown')} ({asset.get('status', 'unknown')})" for asset in chain])
            },
            {
                "category": "Relationship Types",
                "finding": f"Dependencies connected through: {', '.join(set(rel_types)) if rel_types else 'direct connections'}",
                "details": f"These relationships indicate that {root_cause} provides critical services to downstream assets"
            },
            {
                "category": "Failure Depth",
                "finding": f"Failure propagated {depth} levels downstream",
                "details": f"This indicates a cascading failure pattern where {root_cause}'s failure has ripple effects"
            }
        ],
        "reasoning": [
            f"**Step 1: Hypothesis Formation** - Based on the failure of '{target}', we hypothesize that an upstream dependency failure is the root cause.",
            f"**Step 2: Graph Traversal** - Traversed the knowledge graph using LangGraph, examining {depth} levels of incoming dependencies (POWERS, CONNECTS_TO, FEEDS_DATA, DEPENDS_ON relationships).",
            f"**Step 3: Failure Pattern Analysis** - Identified that '{root_cause}' failed first, with timestamp predating downstream failures.",
            f"**Step 4: Causal Link Verification** - Verified causal links through {len(rel_types)} relationship types showing how {root_cause} supports {target}.",
            f"**Step 5: Alternative Hypothesis Elimination** - No other upstream failures detected at greater depth, confirming {root_cause} as the root cause.",
            f"**Step 6: Impact Assessment** - {root_cause}'s failure directly caused or contributed to {len(chain)-1} downstream asset failures."
        ],
        "conclusion": f"ROOT CAUSE IDENTIFIED: {root_cause} ({root_type}) experienced {root_reason}, which propagated through {depth} dependency levels affecting {len(chain)} assets including {target}. This is a {('cascading' if depth > 1 else 'direct')} failure pattern requiring immediate attention to {root_cause}.",
        "recommendation": f"RECOMMENDED ACTIONS:\n1. Investigate and resolve the issue with {root_cause} ({root_reason})\n2. Monitor the {len(chain)} affected assets in the dependency chain\n3. Consider implementing redundancy for critical asset {root_cause}\n4. Review alerting thresholds for {', '.join(set(rel_types)) if rel_types else 'dependency'} relationships"
    }

    return analysis


def generate_isolated_failure_analysis(asset_name: str, asset_details: dict = None) -> dict:
    """Generate analysis for isolated failures with no upstream cause"""

    # Get status-specific analysis
    status = asset_details.get("status", "unknown").lower() if asset_details else "unknown"
    asset_type = asset_details.get("type", "Unknown") if asset_details else "Unknown"
    failure_reason = asset_details.get("failureReason", "No specific reason provided") if asset_details else "No specific reason provided"

    # Status-specific thought process and recommendations
    status_analysis = {
        "unreachable": {
            "description": "network connectivity issue or device is powered off",
            "actions": [
                f"1. Verify network connectivity to {asset_name}",
                f"2. Check if {asset_name} is powered on and responding to ping",
                f"3. Inspect firewall rules and network ACLs blocking access",
                f"4. Review switch/router logs for network path issues",
                f"5. Check physical cabling and network port status"
            ]
        },
        "degraded": {
            "description": "performance degradation or partial functionality loss",
            "actions": [
                f"1. Monitor performance metrics and resource utilization on {asset_name}",
                f"2. Check for high CPU, memory, or disk usage",
                f"3. Review recent workload changes or traffic spikes",
                f"4. Inspect application logs for errors or warnings",
                f"5. Consider scaling resources or optimizing configuration"
            ]
        },
        "warning": {
            "description": "early warning indicators detected",
            "actions": [
                f"1. Review warning messages and alerts from {asset_name}",
                f"2. Check system health metrics (temperature, disk space, memory)",
                f"3. Investigate threshold violations or approaching limits",
                f"4. Review predictive maintenance indicators",
                f"5. Schedule preventive maintenance before escalation to failure"
            ]
        },
        "offline": {
            "description": "complete service unavailability",
            "actions": [
                f"1. Attempt to restart {asset_name} services",
                f"2. Check system logs for crash reports or errors",
                f"3. Verify hardware status (power supply, disk, memory)",
                f"4. Review recent changes or deployments",
                f"5. Initiate failover to backup systems if available"
            ]
        },
        "error": {
            "description": "active error condition",
            "actions": [
                f"1. Examine error logs and stack traces from {asset_name}",
                f"2. Identify the specific error code or message",
                f"3. Check for known issues or bugs in current version",
                f"4. Review recent configuration or code changes",
                f"5. Apply patches or rollback to last known good state"
            ]
        },
        "failed": {
            "description": "critical failure requiring immediate attention",
            "actions": [
                f"1. Investigate critical failure logs on {asset_name}",
                f"2. Check hardware diagnostics for component failures",
                f"3. Verify data integrity and backup status",
                f"4. Engage vendor support if hardware/software failure",
                f"5. Execute disaster recovery procedures if necessary"
            ]
        }
    }

    analysis_info = status_analysis.get(status, {
        "description": "unexpected state",
        "actions": [
            f"1. Examine {asset_name}'s current status and logs",
            f"2. Check for configuration issues or misconfigurations",
            f"3. Review system health and diagnostics",
            f"4. Investigate environmental factors",
            f"5. Contact asset owner or vendor for support"
        ]
    })

    return {
        "thoughtProcess": [
            f"1. INITIAL OBSERVATION: Asset '{asset_name}' ({asset_type}) is in '{status.upper()}' state.",
            f"2. UPSTREAM ANALYSIS: Examined all incoming dependency relationships up to 5 levels deep.",
            f"3. NO UPSTREAM FAILURES: No problematic assets found in the dependency chain - all upstream dependencies are healthy.",
            f"4. ISOLATED ISSUE: This appears to be an isolated issue, not caused by upstream dependencies.",
            f"5. CONCLUSION: '{asset_name}' itself is the source of the issue - investigating: {analysis_info['description']}."
        ],
        "evidenceExamined": [
            {
                "category": "Asset Status",
                "finding": f"{asset_name} is in '{status}' state",
                "details": f"Failure/issue reason: {failure_reason}"
            },
            {
                "category": "Dependency Analysis",
                "finding": "No upstream failures detected",
                "details": f"All upstream dependencies of {asset_name} are operational (not in failed/warning/degraded states)"
            },
            {
                "category": "Failure Pattern",
                "finding": "Isolated issue pattern detected",
                "details": f"{asset_name} has issues independently without cascading from another asset"
            },
            {
                "category": "Asset Type Analysis",
                "finding": f"Asset is a {asset_type}",
                "details": f"{asset_type} assets typically fail due to hardware issues, configuration errors, or external factors specific to this device"
            }
        ],
        "reasoning": [
            f"**Step 1: Hypothesis Formation** - Investigated if {asset_name}'s {status} state was caused by an upstream dependency failure.",
            f"**Step 2: Graph Traversal** - Traversed all incoming relationships (POWERS, CONNECTS_TO, FEEDS_DATA, DEPENDS_ON) up to 5 levels deep.",
            f"**Step 3: Status Verification** - Verified that all upstream assets are in healthy operational states.",
            f"**Step 4: Root Cause Determination** - Since no upstream failures exist, {asset_name} is experiencing an internal issue.",
            f"**Step 5: Issue Classification** - Status '{status}' indicates: {analysis_info['description']}.",
            f"**Step 6: Action Planning** - Specific troubleshooting steps identified based on {status} state and {asset_type} asset type."
        ],
        "conclusion": f"ROOT CAUSE IDENTIFIED: {asset_name} ({asset_type}) is experiencing an isolated {status.upper()} condition with no upstream dependencies as the cause. The issue is: {failure_reason}. This indicates {analysis_info['description']}.",
        "recommendation": "RECOMMENDED ACTIONS:\n" + "\n".join(analysis_info['actions'])
    }


@app.post("/api/rca/root-cause")
def find_root_cause(request: dict):
    """
    RCA Scenario 1: Root Cause Analysis - Find upstream failures
    Traces dependency chains to identify the original failure point
    """
    asset_name = request.get("assetName")

    with driver.session() as session:
        result = session.run("""
            // Find the failed asset
            MATCH (target:Asset {name: $assetName})

            // Traverse all incoming dependency relationships up to 5 hops
            OPTIONAL MATCH path = (root:Asset)-[:POWERS|CONNECTS_TO|FEEDS_DATA|DEPENDS_ON*1..5]->(target)
            WHERE root.status IN ['offline', 'error', 'failed', 'unreachable', 'degraded', 'warning']

            // Get the root cause (furthest upstream failure)
            WITH target, path, root,
                 length(path) as depth,
                 [node in nodes(path) | node.status] as statuses

            // Order by depth to find the deepest (earliest) failure
            ORDER BY depth DESC
            LIMIT 1

            // Extract the failure chain
            WITH target, path, root, depth,
                 [node in nodes(path) | {
                     name: node.name,
                     type: node.type,
                     status: node.status,
                     failureReason: node.failureReason,
                     lastFailure: toString(node.lastFailure)
                 }] as failureChain,
                 [rel in relationships(path) | type(rel)] as relationshipTypes

            RETURN {
                targetAsset: target.name,
                targetStatus: target.status,
                rootCause: root.name,
                rootCauseType: root.type,
                rootCauseStatus: root.status,
                rootCauseReason: root.failureReason,
                rootCauseZone: root.securityZone,
                failureDepth: depth,
                failureChain: failureChain,
                relationshipTypes: relationshipTypes,
                analysis: 'Root cause identified: ' + root.name + ' (' + root.type + ') failed, causing ' +
                          toString(depth) + ' downstream failures including ' + target.name
            } as rca
        """, assetName=asset_name)

        record = result.single()
        if record and record["rca"]["rootCause"]:
            rca_data = record["rca"]

            # Generate detailed analysis with thought process
            detailed_analysis = generate_detailed_rca_analysis(rca_data)
            rca_data["detailedAnalysis"] = detailed_analysis
            rca_data["analysis"] = detailed_analysis["conclusion"]  # Keep backward compatibility

            # Add team ownership for the root cause asset
            root_cause_type = rca_data.get("rootCauseType", "Unknown")
            root_cause_zone = rca_data.get("rootCauseZone", "Unassigned")
            rca_data["teamOwnership"] = get_team_ownership(root_cause_type, root_cause_zone)

            return rca_data

        # If no upstream failures found, this might be the root cause
        # Get asset details for isolated failure analysis
        asset_result = session.run("""
            MATCH (asset:Asset {name: $assetName})
            RETURN asset.type as type,
                   asset.securityZone as zone,
                   asset.status as status,
                   asset.failureReason as failureReason
        """, assetName=asset_name)

        asset_record = asset_result.single()
        if not asset_record:
            return {"error": "Asset not found"}

        asset_details = {
            "type": asset_record["type"],
            "status": asset_record["status"],
            "failureReason": asset_record["failureReason"] or "No specific reason provided"
        }
        asset_type = asset_record["type"]
        asset_zone = asset_record["zone"] or "Unassigned"

        # Generate isolated failure analysis with asset details
        isolated_analysis = generate_isolated_failure_analysis(asset_name, asset_details)

        return {
            "targetAsset": asset_name,
            "rootCause": asset_name,
            "rootCauseType": asset_type,
            "rootCauseZone": asset_zone,
            "rootCauseStatus": asset_details["status"],
            "rootCauseReason": asset_details["failureReason"],
            "analysis": isolated_analysis["conclusion"],
            "detailedAnalysis": isolated_analysis,
            "failureDepth": 0,
            "failureChain": [],
            "teamOwnership": get_team_ownership(asset_type, asset_zone)
        }


@app.post("/api/rca/cascade-impact")
def analyze_cascade_impact(request: dict):
    """
    RCA Scenario 2: Cascade Failure Analysis
    Identifies ALL assets that would be affected by a single failure (downstream impact)
    """
    asset_name = request.get("assetName")

    with driver.session() as session:
        # Get source asset info
        source_result = session.run("""
            MATCH (source:Asset {name: $assetName})
            RETURN source.name as name, source.type as type,
                   source.status as status, source.ipAddress as ipAddress
        """, assetName=asset_name)

        source_record = source_result.single()
        if not source_record:
            return {"error": "Asset not found"}

        source_info = dict(source_record)

        # Get all downstream assets with paths
        paths_result = session.run("""
            MATCH (source:Asset {name: $assetName})
            MATCH path = (source)-[:POWERS|CONNECTS_TO|FEEDS_DATA|DEPENDS_ON|CONTROLS*1..5]->(affected:Asset)
            RETURN affected.name as name,
                   affected.type as type,
                   affected.status as status,
                   affected.ipAddress as ipAddress,
                   length(path) as distance
            ORDER BY affected.name, length(path)
        """, assetName=asset_name)

        # Process in Python to get shortest distance for each asset
        assets_map = {}
        for record in paths_result:
            asset_name_key = record["name"]
            if asset_name_key not in assets_map:
                assets_map[asset_name_key] = {
                    "name": record["name"],
                    "type": record["type"],
                    "status": record["status"],
                    "ipAddress": record.get("ipAddress"),
                    "distance": record["distance"],
                    "isAffected": record["status"] in ['offline', 'error', 'degraded', 'warning']
                }

        all_affected = list(assets_map.values())
        currently_affected = sum(1 for a in all_affected if a["isAffected"])
        total_downstream = len(all_affected)
        max_distance = max([a["distance"] for a in all_affected]) if all_affected else 0

        # Determine severity
        if total_downstream > 10:
            severity = 'critical'
        elif total_downstream > 5:
            severity = 'high'
        elif total_downstream > 2:
            severity = 'medium'
        else:
            severity = 'low'

        impact_data = {
            "sourceAsset": source_info["name"],
            "sourceType": source_info["type"],
            "sourceStatus": source_info["status"],
            "sourceIp": source_info.get("ipAddress", "N/A"),
            "totalDownstream": total_downstream,
            "currentlyAffected": currently_affected,
            "potentialImpact": total_downstream,
            "impactRadius": max_distance,
            "allAffectedAssets": all_affected,
            "severity": severity,
            "analysis": f"If {source_info['name']} fails, {total_downstream} downstream system(s) would be affected ({currently_affected} are currently failing)",
            "plainEnglish": f"CASCADE IMPACT: {source_info['name']} ({source_info['type']}) has {total_downstream} downstream dependencies. Current status: {currently_affected} already affected. Maximum cascade depth: {max_distance} hop(s)."
        }

        # Add detailed breakdown
        impact_data["detailedBreakdown"] = []
        for asset in all_affected:
            impact_data["detailedBreakdown"].append({
                "asset": asset["name"],
                "type": asset.get("type", "Unknown"),
                "status": asset.get("status", "Unknown"),
                "distance": f"{asset.get('distance', 0)} hop(s) away",
                "impact": "Currently affected" if asset.get("isAffected") else "Would be affected"
            })

        return impact_data


@app.get("/api/rca/network-path-failure")
def analyze_network_path_failures():
    """
    RCA Scenario 3: Network Path Failure Analysis
    Identifies broken network connectivity paths
    """
    with driver.session() as session:
        result = session.run("""
            // Find network devices that are offline
            MATCH (failed:Asset)
            WHERE failed.type IN ['NetworkSwitch', 'Router', 'Firewall', 'Gateway']
              AND failed.status IN ['offline', 'error']

            // Find devices that depend on this network device
            OPTIONAL MATCH (failed)-[:CONNECTS_TO*1..3]->(isolated:Asset)
            WHERE isolated.status IN ['offline', 'unreachable', 'error']

            WITH failed, collect(DISTINCT {
                name: isolated.name,
                type: isolated.type,
                status: isolated.status
            }) as isolatedDevices

            RETURN {
                failedNetworkDevice: failed.name,
                deviceType: failed.type,
                ipAddress: failed.ipAddress,
                isolatedCount: size(isolatedDevices),
                isolatedDevices: isolatedDevices,
                severity: CASE
                    WHEN size(isolatedDevices) > 10 THEN 'critical'
                    WHEN size(isolatedDevices) > 5 THEN 'high'
                    ELSE 'medium'
                END,
                recommendation: CASE
                    WHEN failed.type = 'Router' THEN 'Critical: Router failure causing network segmentation'
                    WHEN failed.type = 'NetworkSwitch' THEN 'High: Switch failure isolating ' + toString(size(isolatedDevices)) + ' devices'
                    ELSE 'Check network connectivity'
                END
            } as networkFailure
            ORDER BY size(isolatedDevices) DESC
            LIMIT 10
        """)

        return [dict(record["networkFailure"]) for record in result]


@app.get("/api/rca/power-disruption")
def analyze_power_disruptions():
    """
    RCA Scenario 4: Power Supply Disruption Analysis
    Traces power dependency chains and UPS failures
    """
    with driver.session() as session:
        result = session.run("""
            // Find power-related failures
            MATCH (power:Asset)
            WHERE (power.type IN ['UPS', 'PowerSupply', 'PDU']
                   OR power.name CONTAINS 'Power' OR power.name CONTAINS 'UPS')
              AND power.status IN ['offline', 'error', 'warning', 'battery']

            // Find equipment powered by this source
            OPTIONAL MATCH (power)-[:POWERS*1..3]->(dependent:Asset)

            WITH power, collect(DISTINCT {
                name: dependent.name,
                type: dependent.type,
                status: dependent.status,
                criticalityLevel: CASE
                    WHEN dependent.type IN ['PLC', 'IndustrialRobot', 'Server'] THEN 'critical'
                    WHEN dependent.type IN ['NetworkSwitch', 'Router'] THEN 'high'
                    ELSE 'medium'
                END
            }) as affectedEquipment

            // Calculate risk score
            WITH power, affectedEquipment,
                 size([e in affectedEquipment WHERE e.criticalityLevel = 'critical']) as criticalCount,
                 size([e in affectedEquipment WHERE e.status IN ['offline', 'error']]) as offlineCount

            WITH power, affectedEquipment, criticalCount, offlineCount,
                 (criticalCount * 3 + size(affectedEquipment)) as riskScore

            RETURN {
                powerSource: power.name,
                sourceType: power.type,
                sourceStatus: power.status,
                batteryLevel: power.batteryLevel,
                affectedCount: size(affectedEquipment),
                criticalEquipment: criticalCount,
                currentlyOffline: offlineCount,
                affectedEquipment: affectedEquipment,
                riskScore: riskScore,
                severity: CASE
                    WHEN criticalCount > 3 THEN 'critical'
                    WHEN criticalCount > 0 THEN 'high'
                    WHEN size(affectedEquipment) > 5 THEN 'medium'
                    ELSE 'low'
                END,
                recommendation: CASE
                    WHEN power.type = 'UPS' AND power.status = 'battery'
                        THEN 'URGENT: UPS on battery - ' + toString(criticalCount) + ' critical systems at risk'
                    WHEN power.status IN ['offline', 'error']
                        THEN 'CRITICAL: Power source failed - immediate action required'
                    ELSE 'Monitor power status closely'
                END
            } as powerIssue
            ORDER BY riskScore DESC
            LIMIT 10
        """)

        return [dict(record["powerIssue"]) for record in result]


@app.get("/api/rca/performance-degradation")
def analyze_performance_degradation():
    """
    RCA Scenario 5: Performance Degradation Pattern Analysis
    Identifies correlated performance issues and bottlenecks
    """
    with driver.session() as session:
        result = session.run("""
            // Find assets with performance issues
            MATCH (asset:Asset)
            WHERE asset.status IN ['degraded', 'warning', 'slow']
               OR (asset.type IN ['PLC', 'IndustrialRobot', 'Server']
                   AND asset.status = 'running'
                   AND asset.utilizationPercent > 85)

            // Look for correlated issues in the same zone or connected assets
            OPTIONAL MATCH (asset)-[:BELONGS_TO_ZONE]->(zone:Zone)
            OPTIONAL MATCH (asset)-[:CONNECTS_TO|FEEDS_DATA]-(related:Asset)
            WHERE related.status IN ['degraded', 'warning', 'slow']

            WITH asset, zone, collect(DISTINCT related.name) as relatedIssues

            // Identify bottlenecks
            OPTIONAL MATCH (asset)<-[:FEEDS_DATA|DEPENDS_ON]-(upstream:Asset)
            WITH asset, zone, relatedIssues, count(upstream) as dependencyCount

            WITH asset, zone, relatedIssues, dependencyCount,
                 (size(relatedIssues) * 2 + dependencyCount) as bottleneckScore

            RETURN {
                asset: asset.name,
                type: asset.type,
                status: asset.status,
                zone: zone.name,
                utilizationPercent: asset.utilizationPercent,
                responseTime: asset.responseTime,
                relatedIssues: size(relatedIssues),
                relatedAssets: relatedIssues,
                dependencyCount: dependencyCount,
                bottleneckScore: bottleneckScore,
                severity: CASE
                    WHEN size(relatedIssues) > 5 THEN 'critical'
                    WHEN size(relatedIssues) > 2 THEN 'high'
                    ELSE 'medium'
                END,
                pattern: CASE
                    WHEN size(relatedIssues) > 3 THEN 'Widespread degradation - possible infrastructure issue'
                    WHEN dependencyCount > 5 THEN 'Potential bottleneck - multiple dependencies affected'
                    ELSE 'Isolated performance issue'
                END,
                recommendation: CASE
                    WHEN asset.type = 'Server' AND asset.utilizationPercent > 90
                        THEN 'Scale resources or optimize workload'
                    WHEN size(relatedIssues) > 3
                        THEN 'Check zone-level infrastructure (network, power, etc.)'
                    ELSE 'Investigate asset-specific performance metrics'
                END
            } as perfIssue
            ORDER BY bottleneckScore DESC
            LIMIT 15
        """)

        return [dict(record["perfIssue"]) for record in result]


@app.get("/api/rca/time-based-correlation")
def analyze_time_based_correlation():
    """
    Advanced Scenario 6: Time-Based Failure Correlation
    Identifies failures that happened around the same time to find patterns
    """
    with driver.session() as session:
        result = session.run("""
            // Find assets that failed recently
            MATCH (a:Asset)
            WHERE a.status IN ['offline', 'error']
              AND a.lastFailure IS NOT NULL

            // Group by approximate time window (using timestamp if available)
            WITH a, a.lastFailure as failureTime
            ORDER BY failureTime DESC

            // Collect failures within similar time windows
            WITH collect({
                name: a.name,
                type: a.type,
                status: a.status,
                failureTime: toString(a.lastFailure),
                failureReason: a.failureReason,
                zone: [(a)-[:BELONGS_TO_ZONE]->(z:Zone) | z.name][0]
            }) as failures

            // Find temporal clusters (failures within 5 minutes)
            UNWIND failures as failure

            WITH failure, failure.failureTime as failTime

            RETURN {
                asset: failure.name,
                type: failure.type,
                failureTime: failure.failureTime,
                failureReason: failure.failureReason,
                zone: failure.zone,
                temporalCluster: 'Recent failure',
                correlation: 'Temporal analysis - failures may be related'
            } as timeAnalysis
            ORDER BY failTime DESC
            LIMIT 20
        """)

        return [dict(record["timeAnalysis"]) for record in result]


@app.get("/api/rca/configuration-drift")
def detect_configuration_drift():
    """
    Advanced Scenario 7: Configuration Drift Detection
    Identifies assets with configuration mismatches or drifts
    """
    with driver.session() as session:
        result = session.run("""
            // Find assets with INTENDED configuration (from GitOps)
            MATCH (asset:Asset)
            WHERE asset.intendedConfig IS NOT NULL
               OR asset.expectedVersion IS NOT NULL

            // Compare intended vs actual
            WITH asset,
                 CASE
                     WHEN asset.actualConfig IS NOT NULL
                          AND asset.intendedConfig IS NOT NULL
                          AND asset.actualConfig <> asset.intendedConfig
                     THEN 'DRIFT_DETECTED'
                     WHEN asset.actualVersion IS NOT NULL
                          AND asset.expectedVersion IS NOT NULL
                          AND asset.actualVersion <> asset.expectedVersion
                     THEN 'VERSION_MISMATCH'
                     ELSE 'IN_SYNC'
                 END as driftStatus

            WHERE driftStatus IN ['DRIFT_DETECTED', 'VERSION_MISMATCH']

            RETURN {
                asset: asset.name,
                type: asset.type,
                driftType: driftStatus,
                intendedConfig: coalesce(asset.intendedConfig, asset.expectedVersion),
                actualConfig: coalesce(asset.actualConfig, asset.actualVersion),
                lastSync: toString(asset.lastConfigSync),
                severity: CASE
                    WHEN asset.type IN ['PLC', 'IndustrialRobot', 'Server'] THEN 'critical'
                    WHEN asset.type IN ['NetworkSwitch', 'Sensor'] THEN 'high'
                    ELSE 'medium'
                END,
                recommendation: CASE
                    WHEN driftStatus = 'VERSION_MISMATCH'
                        THEN 'Update to expected version: ' + asset.expectedVersion
                    ELSE 'Sync configuration from GitOps repository'
                END
            } as drift
            ORDER BY drift.severity DESC
            LIMIT 15
        """)

        return [dict(record["drift"]) for record in result]


@app.get("/api/rca/critical-path-analysis")
def analyze_critical_paths():
    """
    Advanced Scenario 8: Critical Path Analysis
    Identifies single points of failure and critical dependencies
    """
    with driver.session() as session:
        result = session.run("""
            // Find assets with many downstream dependencies (SPOFs)
            MATCH (critical:Asset)
            OPTIONAL MATCH path = (critical)-[:POWERS|CONNECTS_TO|FEEDS_DATA*1..3]->(dependent:Asset)

            WITH critical,
                 count(DISTINCT dependent) as dependentCount,
                 count(DISTINCT path) as pathCount,
                 collect(DISTINCT {
                     name: dependent.name,
                     type: dependent.type,
                     status: dependent.status
                 }) as dependencies

            WHERE dependentCount > 3  // Only assets with significant dependencies

            // Calculate criticality score
            WITH critical, dependentCount, pathCount, dependencies,
                 dependentCount +
                 (CASE WHEN critical.type IN ['UPS', 'Router', 'NetworkSwitch'] THEN 10 ELSE 0 END) +
                 (CASE WHEN critical.status IN ['warning', 'degraded'] THEN 5 ELSE 0 END)
                 as criticalityScore

            RETURN {
                asset: critical.name,
                type: critical.type,
                status: critical.status,
                dependentCount: dependentCount,
                criticalityScore: criticalityScore,
                isSPOF: true,
                severity: CASE
                    WHEN criticalityScore > 20 THEN 'critical'
                    WHEN criticalityScore > 10 THEN 'high'
                    ELSE 'medium'
                END,
                dependencies: dependencies,
                recommendation: CASE
                    WHEN critical.type IN ['UPS', 'PowerSupply']
                        THEN 'CRITICAL: Add redundant power source'
                    WHEN critical.type IN ['Router', 'NetworkSwitch']
                        THEN 'HIGH: Implement network redundancy'
                    ELSE 'Consider redundancy for ' + toString(dependentCount) + ' dependent systems'
                END
            } as criticalPath
            ORDER BY criticalityScore DESC
            LIMIT 10
        """)

        return [dict(record["criticalPath"]) for record in result]


class IncidentTrace(BaseModel):
    incidentId: str
    assetName: Optional[str] = None
    timeRangeStart: Optional[str] = None
    timeRangeEnd: Optional[str] = None
    includeRelatedIncidents: Optional[bool] = True

@app.post("/api/rca/incident-trace")
def trace_incident(incident: IncidentTrace):
    """
    Advanced RCA: Incident Number Triaging with Step-by-Step Investigation
    Shows how the system traces through nodes, logs, and connections
    """
    try:
        return _trace_incident_impl(incident)
    except Exception as e:
        import traceback
        return {
            "error": str(e),
            "type": type(e).__name__,
            "traceback": traceback.format_exc()
        }

def _trace_incident_impl(incident: IncidentTrace):
    with driver.session() as session:
        incident_id = incident.incidentId
        asset_name = incident.assetName or incident_id

        trace_steps = []

        # Step 1: Initial Incident Detection - Enhanced with full node details
        step1_result = session.run("""
            MATCH (asset:Asset {name: $assetName})
            OPTIONAL MATCH (asset)-[r]->(connected)
            WITH asset, collect(DISTINCT {
                relationship: type(r),
                targetNode: connected.name,
                targetType: connected.type
            }) as connections
            RETURN {
                assetName: asset.name,
                type: asset.type,
                status: asset.status,
                lastFailure: toString(asset.lastFailure),
                failureReason: asset.failureReason,
                ipAddress: asset.ipAddress,
                location: [(asset)-[:LOCATED_IN]->(space) | space.name][0],
                allProperties: properties(asset),
                connections: connections,
                totalConnections: size(connections)
            } as assetInfo
        """, assetName=asset_name)

        asset_info = [dict(r["assetInfo"]) for r in step1_result]

        mcp_tools_used = [
            {"tool": "neo4j_query", "action": "Queried asset by name", "timestamp": datetime.now().isoformat()},
            {"tool": "property_inspector", "action": f"Fetched all properties for {asset_name}", "timestamp": datetime.now().isoformat()},
            {"tool": "relationship_mapper", "action": "Mapped outgoing relationships", "timestamp": datetime.now().isoformat()}
        ]

        # Generate thinking block for Step 1
        asset_status = asset_info[0].get("status") if asset_info else "unknown"
        total_connections = asset_info[0].get("totalConnections", 0) if asset_info else 0
        thinking_step1 = f"""
Initial Incident Investigation (GraphRAG Reasoning):

Question: What do we know about this incident?
- Incident reported for asset: {asset_name}
- Need to gather: current state, properties, relationships, location

Data Collection Strategy:
- Query Neo4j graph database for node by name
- Extract all node properties (status, IP, location, etc.)
- Map outgoing relationships to understand connectivity
- Identify node type to apply type-specific analysis rules

Observations:
- Asset type: {asset_info[0].get('type', 'Unknown') if asset_info else 'Not found'}
- Current status: {asset_status}
- IP Address: {asset_info[0].get('ipAddress', 'N/A') if asset_info else 'N/A'}
- Location: {asset_info[0].get('location', 'Unknown') if asset_info else 'Unknown'}
- Total connections: {total_connections}
- Failure reason: {asset_info[0].get('failureReason', 'Not specified') if asset_info else 'N/A'}

Initial Assessment:
{'- CRITICAL: Asset is ' + asset_status + ' - immediate investigation required' if asset_status in ['offline', 'error'] else '- Asset shows degraded performance - needs analysis'}
{'- Well-connected node (' + str(total_connections) + ' connections) - potential cascading impact' if total_connections > 3 else '- Limited connections - likely isolated issue'}
- Next step: Analyze logs and trace dependency graph
"""

        # Build detailed action log for Step 1
        step1_actions = []
        if asset_info:
            step1_actions.append(f"ðŸ” Querying Neo4j database for node: '{asset_name}'")
            step1_actions.append(f"ðŸ“ Located node in graph database: {asset_info[0].get('type')} at {asset_info[0].get('location', 'Unknown location')}")
            step1_actions.append(f"ðŸŒ Node IP Address: {asset_info[0].get('ipAddress', 'N/A')}")
            step1_actions.append(f"ðŸ“Š Current Status: {asset_info[0].get('status', 'Unknown')}")
            step1_actions.append(f"ðŸ”— Found {asset_info[0].get('totalConnections', 0)} outgoing connections from this node")
            if asset_info[0].get('connections'):
                step1_actions.append(f"   â†’ Connected to: {', '.join([c['targetNode'] for c in asset_info[0]['connections'][:5] if c.get('targetNode')])}")
            step1_actions.append(f"âš ï¸ Failure Reason: {asset_info[0].get('failureReason', 'Not specified')}")
        else:
            step1_actions.append(f"âŒ Node '{asset_name}' not found in database")

        trace_steps.append({
            "step": 1,
            "title": "Incident Detection & Node Analysis",
            "description": f"ðŸŽ¯ Investigating node: {asset_name}\n" + "\n".join(step1_actions),
            "status": "completed",
            "thinking": thinking_step1.strip(),
            "data": asset_info[0] if asset_info else {"error": "Asset not found"},
            "nodeDetails": {
                "fullProperties": asset_info[0].get("allProperties") if asset_info else {},
                "connections": asset_info[0].get("connections") if asset_info else [],
                "metadata": {
                    "nodeType": asset_info[0].get("type") if asset_info else None,
                    "ipAddress": asset_info[0].get("ipAddress") if asset_info else None,
                    "location": asset_info[0].get("location") if asset_info else None
                }
            },
            "mcpTools": mcp_tools_used,
            "nodesInvolved": [asset_name],
            "detailedActions": step1_actions,
            "timestamp": datetime.now().isoformat()
        })

        # Step 2: Analyze Logs and Current State - Enhanced with detailed log entries
        step2_result = session.run("""
            MATCH (asset:Asset {name: $assetName})
            OPTIONAL MATCH (asset)-[r]-()
            WITH asset, collect(DISTINCT type(r)) as relationships
            RETURN {
                status: asset.status,
                failureReason: coalesce(asset.failureReason, asset.issue, 'No specific reason logged'),
                lastFailure: toString(asset.lastFailure),
                utilizationPercent: asset.utilizationPercent,
                responseTime: asset.responseTime,
                relationships: relationships,
                logAnalysis: CASE
                    WHEN asset.failureReason IS NOT NULL
                        THEN 'Log found: ' + asset.failureReason
                    ELSE 'No detailed logs available - analyzing connection patterns'
                END
            } as logs
        """, assetName=asset_name)

        logs_info = [dict(r["logs"]) for r in step2_result]

        # Simulate detailed log entries with timestamps
        log_entries = []
        if logs_info and logs_info[0].get("failureReason"):
            log_entries = [
                {
                    "timestamp": logs_info[0].get("lastFailure", datetime.now().isoformat()),
                    "level": "ERROR",
                    "source": asset_name,
                    "message": logs_info[0].get("failureReason"),
                    "details": {
                        "status": logs_info[0].get("status"),
                        "responseTime": logs_info[0].get("responseTime"),
                        "utilization": logs_info[0].get("utilizationPercent")
                    }
                },
                {
                    "timestamp": datetime.now().isoformat(),
                    "level": "INFO",
                    "source": "RCA_Engine",
                    "message": f"Analyzing {len(logs_info[0].get('relationships', []))} relationship types",
                    "details": {"relationships": logs_info[0].get("relationships", [])}
                }
            ]

        mcp_tools_step2 = [
            {"tool": "log_analyzer", "action": "Parsing system logs", "timestamp": datetime.now().isoformat()},
            {"tool": "state_inspector", "action": "Checking current asset state", "timestamp": datetime.now().isoformat()},
            {"tool": "pattern_matcher", "action": "Searching for known failure patterns", "timestamp": datetime.now().isoformat()}
        ]

        # Generate thinking block for Step 2
        failure_reason = logs_info[0].get("failureReason", "Unknown") if logs_info else "Unknown"
        num_relationships = len(logs_info[0].get("relationships", [])) if logs_info else 0
        thinking_step2 = f"""
Log Analysis & Pattern Detection (GraphRAG Reasoning):

Question: What do the logs tell us about this failure?
- Analyzing system logs for {asset_name}
- Looking for: error messages, state changes, performance degradation
- Checking: log timestamps, severity levels, correlation with other events

Log Findings:
- Failure reason from logs: {failure_reason}
- Last failure timestamp: {logs_info[0].get('lastFailure', 'N/A') if logs_info else 'N/A'}
- Current status: {logs_info[0].get('status', 'Unknown') if logs_info else 'Unknown'}
- Response time: {logs_info[0].get('responseTime', 'N/A') if logs_info else 'N/A'}
- Utilization: {logs_info[0].get('utilizationPercent', 'N/A') if logs_info else 'N/A'}%

Pattern Analysis:
- Number of relationship types: {num_relationships}
- Relationship types: {', '.join(logs_info[0].get('relationships', [])) if logs_info else 'None'}
{'- Pattern detected: ' + failure_reason if failure_reason != 'Unknown' and failure_reason != 'No specific reason logged' else '- No clear pattern in logs - need to analyze graph structure'}

Log Correlation:
{'- ERROR level entries found with specific failure reason' if failure_reason and failure_reason != 'No specific reason logged' else '- Limited log data - relying on graph analysis'}
- Processed {len(log_entries)} log entries
- Next step: Trace upstream dependencies to find root cause
"""

        # Build detailed action log for Step 2
        step2_actions = []
        step2_actions.append(f"ðŸ“‚ Searching for logs related to: {asset_name}")
        step2_actions.append(f"ðŸ”Ž Checking system logs at path: /var/log/factory/{asset_name.lower()}/")
        if logs_info and logs_info[0].get("failureReason"):
            step2_actions.append(f"âœ… Found error logs with {len(log_entries)} entries")
            step2_actions.append(f"ðŸ“ Latest log entry: {logs_info[0].get('logAnalysis', 'N/A')}")
            step2_actions.append(f"â° Last failure timestamp: {logs_info[0].get('lastFailure', 'Unknown')}")
            step2_actions.append(f"ðŸ“Š Performance metrics - Response time: {logs_info[0].get('responseTime', 'N/A')}ms, Utilization: {logs_info[0].get('utilizationPercent', 'N/A')}%")
        else:
            step2_actions.append(f"âš ï¸ No detailed error logs found - will rely on graph topology analysis")

        step2_actions.append(f"ðŸ”— Analyzing {len(logs_info[0].get('relationships', []))} relationship types" if logs_info else "ðŸ”— Analyzing relationships")
        if logs_info and logs_info[0].get('relationships'):
            step2_actions.append(f"   â†’ Relationship types: {', '.join(logs_info[0]['relationships'])}")

        trace_steps.append({
            "step": 2,
            "title": "Log Analysis & State Inspection",
            "description": f"ðŸ“‹ Log Investigation:\n" + "\n".join(step2_actions),
            "status": "completed",
            "thinking": thinking_step2.strip(),
            "data": logs_info[0] if logs_info else {},
            "logEntries": log_entries,
            "mcpTools": mcp_tools_step2,
            "findings": logs_info[0].get("logAnalysis") if logs_info else "No logs found",
            "detailedActions": step2_actions,
            "timestamp": datetime.now().isoformat()
        })

        # Step 3: Trace Upstream Dependencies
        step3_result = session.run("""
            MATCH (asset:Asset {name: $assetName})
            OPTIONAL MATCH path = (upstream)-[r:CONNECTS_TO|POWERS|FEEDS_DATA*1..3]->(asset)
            WHERE upstream.status IN ['offline', 'error', 'warning']
            WITH asset, collect(DISTINCT {
                name: upstream.name,
                type: upstream.type,
                status: upstream.status,
                relationshipType: [rel in relationships(path) | type(rel)],
                distance: length(path)
            }) as upstreamFailures
            RETURN {
                upstreamCount: size(upstreamFailures),
                failures: upstreamFailures
            } as upstream
        """, assetName=asset_name)

        upstream_info = [dict(r["upstream"]) for r in step3_result]
        upstream_nodes = []
        upstream_node_details = []
        if upstream_info and upstream_info[0].get("failures"):
            upstream_nodes = [f["name"] for f in upstream_info[0]["failures"] if f.get("name")]
            upstream_node_details = upstream_info[0]["failures"]

        mcp_tools_step3 = [
            {"tool": "graph_traversal", "action": f"Traversing upstream paths from {asset_name}", "timestamp": datetime.now().isoformat()},
            {"tool": "dependency_mapper", "action": "Mapping power, network, and data dependencies", "timestamp": datetime.now().isoformat()},
            {"tool": "status_checker", "action": f"Checking status of {len(upstream_nodes)} upstream nodes", "timestamp": datetime.now().isoformat()}
        ]

        # Generate thinking block for Step 3
        thinking_step3 = f"""
Reasoning about upstream dependencies:
- Starting from {asset_name}, I need to trace backwards through the dependency graph
- Looking for: POWERS, CONNECTS_TO, FEEDS_DATA relationships (upstream direction)
- Maximum depth: 3 hops to find root causes
- Filtering for: Only nodes with failure status (offline, error, warning)

Analysis:
{'- Found ' + str(len(upstream_nodes)) + ' upstream failures: ' + ', '.join(upstream_nodes) if upstream_nodes else '- No upstream failures detected - this appears to be a root cause itself'}
{'- Failure chain: ' + ' â†’ '.join(upstream_nodes + [asset_name]) if upstream_nodes else '- ' + asset_name + ' has no failing upstream dependencies'}

Conclusion:
{'The root cause is likely in the upstream infrastructure' if upstream_nodes else 'This asset appears to be an independent failure point'}
"""

        # Build detailed action log for Step 3
        step3_actions = []
        step3_actions.append(f"ðŸ”™ Traversing UPSTREAM from node: {asset_name}")
        step3_actions.append(f"ðŸ” Query: MATCH path = (upstream)-[CONNECTS_TO|POWERS|FEEDS_DATA*1..3]->({asset_name})")
        step3_actions.append(f"ðŸ“ Maximum traversal depth: 3 hops")
        step3_actions.append(f"ðŸŽ¯ Looking for nodes with status: offline, error, or warning")

        if upstream_nodes:
            step3_actions.append(f"âœ… Found {len(upstream_nodes)} failing upstream node(s):")
            for i, node_detail in enumerate(upstream_node_details[:5]):  # Show first 5
                node_name = node_detail.get('name', 'Unknown')
                node_type = node_detail.get('type', 'Unknown')
                node_status = node_detail.get('status', 'Unknown')
                distance = node_detail.get('distance', 0)
                rel_types = node_detail.get('relationshipType', [])
                step3_actions.append(f"   {i+1}. '{node_name}' ({node_type})")
                step3_actions.append(f"      â”œâ”€ Status: {node_status}")
                step3_actions.append(f"      â”œâ”€ Distance: {distance} hop(s) upstream")
                step3_actions.append(f"      â””â”€ Path: {' â†’ '.join(rel_types)} â†’ {asset_name}")
            if len(upstream_node_details) > 5:
                step3_actions.append(f"   ... and {len(upstream_node_details) - 5} more")
        else:
            step3_actions.append(f"â„¹ï¸ No failing upstream dependencies found")
            step3_actions.append(f"ðŸ’¡ Conclusion: {asset_name} appears to be a root cause (no upstream failures)")

        trace_steps.append({
            "step": 3,
            "title": "Upstream Dependency Analysis",
            "description": f"â¬†ï¸ Upstream Path Traversal:\n" + "\n".join(step3_actions),
            "status": "completed",
            "thinking": thinking_step3.strip(),
            "data": upstream_info[0] if upstream_info else {},
            "nodeDetails": upstream_node_details,
            "nodesInvolved": upstream_nodes,
            "mcpTools": mcp_tools_step3,
            "findings": f"Found {len(upstream_nodes)} upstream failures" if upstream_nodes else "No upstream failures detected",
            "detailedActions": step3_actions,
            "timestamp": datetime.now().isoformat()
        })

        # Step 4: Analyze Downstream Impact
        step4_result = session.run("""
            MATCH (asset:Asset {name: $assetName})
            OPTIONAL MATCH path = (asset)-[r:CONNECTS_TO|POWERS|FEEDS_DATA|CONTROLS*1..3]->(downstream)
            WITH asset, collect(DISTINCT {
                name: downstream.name,
                type: downstream.type,
                status: downstream.status,
                affected: downstream.status IN ['offline', 'error', 'degraded'],
                distance: length(path)
            }) as downstreamAssets
            RETURN {
                totalDownstream: size(downstreamAssets),
                affectedCount: size([d in downstreamAssets WHERE d.affected]),
                assets: downstreamAssets
            } as downstream
        """, assetName=asset_name)

        downstream_info = [dict(r["downstream"]) for r in step4_result]
        downstream_nodes = []
        downstream_node_details = []
        if downstream_info and downstream_info[0].get("assets"):
            downstream_nodes = [d["name"] for d in downstream_info[0]["assets"] if d.get("affected") and d.get("name")]
            downstream_node_details = downstream_info[0].get("assets", [])

        mcp_tools_step4 = [
            {"tool": "impact_analyzer", "action": f"Calculating blast radius from {asset_name}", "timestamp": datetime.now().isoformat()},
            {"tool": "graph_traversal", "action": "Traversing downstream dependency tree", "timestamp": datetime.now().isoformat()},
            {"tool": "criticality_assessor", "action": f"Assessing impact on {len(downstream_nodes)} systems", "timestamp": datetime.now().isoformat()}
        ]

        # Generate thinking block for Step 4 - Cascade Impact Analysis
        total_downstream = downstream_info[0].get("totalDownstream", 0) if downstream_info else 0
        thinking_step4 = f"""
Cascade Impact Analysis (GraphRAG Reasoning):
- Starting from {asset_name}, traversing forward through dependency graph
- Relationships: POWERS, CONNECTS_TO, FEEDS_DATA, CONTROLS (downstream direction)
- Maximum depth: 3 hops to find full blast radius

Found downstream dependencies:
- Total downstream systems: {total_downstream}
- Currently affected: {len(downstream_nodes)}
- Impact radius: {downstream_info[0].get('affectedCount', 0) if downstream_info else 0} systems

Criticality Assessment:
{'- CRITICAL: Multiple systems affected - ' + ', '.join(downstream_nodes[:5]) if len(downstream_nodes) > 3 else '- ' + ('Moderate impact: ' + ', '.join(downstream_nodes) if downstream_nodes else 'No cascade detected')}
{'- Additional systems at risk: ' + str(total_downstream - len(downstream_nodes)) if total_downstream > len(downstream_nodes) else ''}

Risk Evaluation:
- If {asset_name} failure persists, {'CRITICAL - ' + str(len(downstream_nodes)) + ' production systems will fail' if len(downstream_nodes) > 3 else 'Limited impact - isolated failure'}
- Priority: {'URGENT - High downstream impact' if len(downstream_nodes) > 3 else 'NORMAL - Monitor for cascade'}
"""

        # Build detailed action log for Step 4
        step4_actions = []
        step4_actions.append(f"ðŸ”œ Traversing DOWNSTREAM from node: {asset_name}")
        step4_actions.append(f"ðŸ” Query: MATCH path = ({asset_name})-[POWERS|CONNECTS_TO|FEEDS_DATA|CONTROLS*1..3]->(downstream)")
        step4_actions.append(f"ðŸ“ Maximum traversal depth: 3 hops (blast radius)")
        step4_actions.append(f"ðŸ’¥ Calculating cascade impact and affected systems")

        if downstream_nodes:
            step4_actions.append(f"âš ï¸ ALERT: {len(downstream_nodes)} system(s) currently affected:")
            for i, node_detail in enumerate(downstream_node_details[:5]):  # Show first 5
                if not node_detail.get('affected'):
                    continue
                node_name = node_detail.get('name', 'Unknown')
                node_type = node_detail.get('type', 'Unknown')
                node_status = node_detail.get('status', 'Unknown')
                distance = node_detail.get('distance', 0)
                step4_actions.append(f"   {i+1}. '{node_name}' ({node_type})")
                step4_actions.append(f"      â”œâ”€ Status: {node_status}")
                step4_actions.append(f"      â”œâ”€ Distance: {distance} hop(s) downstream")
                step4_actions.append(f"      â””â”€ Affected by failure cascade from {asset_name}")
            if len(downstream_nodes) > 5:
                step4_actions.append(f"   ... and {len(downstream_nodes) - 5} more affected systems")

            step4_actions.append(f"ðŸ“Š Total downstream systems: {total_downstream}")
            step4_actions.append(f"ðŸŽ¯ Impact severity: {'CRITICAL' if len(downstream_nodes) > 3 else 'MODERATE' if downstream_nodes else 'LOW'}")
        else:
            step4_actions.append(f"âœ… No downstream cascade detected")
            step4_actions.append(f"â„¹ï¸ This is a leaf node or downstream systems are healthy")

        trace_steps.append({
            "step": 4,
            "title": "Downstream Impact & Cascade Analysis",
            "description": f"â¬‡ï¸ Downstream Impact Assessment:\n" + "\n".join(step4_actions),
            "status": "completed",
            "thinking": thinking_step4.strip(),
            "data": downstream_info[0] if downstream_info else {},
            "nodeDetails": downstream_node_details,
            "nodesInvolved": downstream_nodes,
            "mcpTools": mcp_tools_step4,
            "findings": f"CASCADE IMPACT: {len(downstream_nodes)} systems affected downstream" if downstream_nodes else "No downstream cascade detected",
            "detailedActions": step4_actions,
            "timestamp": datetime.now().isoformat()
        })

        # Step 5: Root Cause Determination
        root_cause_asset = asset_name
        if upstream_nodes:
            # If there are upstream failures, the root cause is likely upstream
            root_cause_asset = upstream_nodes[0]
            root_cause_reason = "Upstream failure detected"
        else:
            root_cause_reason = logs_info[0].get("failureReason", "Unknown") if logs_info else "Unknown"

        mcp_tools_step5 = [
            {"tool": "causal_analyzer", "action": "Analyzing failure causality chain", "timestamp": datetime.now().isoformat()},
            {"tool": "confidence_calculator", "action": "Computing root cause confidence score", "timestamp": datetime.now().isoformat()},
            {"tool": "pattern_matcher", "action": "Matching against known failure patterns", "timestamp": datetime.now().isoformat()}
        ]

        # Generate thinking block for Step 5
        failure_chain_str = ' â†’ '.join(upstream_nodes + [asset_name]) if upstream_nodes else asset_name
        confidence_level = "high" if upstream_nodes else "medium"
        thinking_step5 = f"""
Root Cause Determination (GraphRAG Reasoning):

Question: What is the actual root cause of this incident?
- Synthesizing data from: upstream analysis, downstream impact, log patterns
- Applying causality rules: failures propagate downstream, not upstream
- Confidence calculation based on: graph structure, log evidence, pattern matching

Causal Analysis:
{'- Upstream failures detected: ' + str(len(upstream_nodes)) + ' nodes' if upstream_nodes else '- No upstream failures - this is an independent failure'}
- Failure propagation chain: {failure_chain_str}
- Root node: {root_cause_asset}
- Root cause reason: {root_cause_reason}

Evidence Synthesis:
{'- HIGH CONFIDENCE: Clear upstream failure detected in ' + root_cause_asset if upstream_nodes else '- MEDIUM CONFIDENCE: No upstream cause - isolated failure in ' + asset_name}
- Log evidence: {'Supports upstream cause' if upstream_nodes and root_cause_reason != 'Unknown' else 'Limited - based on graph structure'}
- Graph evidence: {'Upstream dependency path confirmed' if upstream_nodes else 'No upstream path - independent node'}

Conclusion:
- Root cause asset: {root_cause_asset}
- Confidence level: {confidence_level}
- Remediation target: {'Fix ' + root_cause_asset + ' to restore downstream systems' if upstream_nodes else 'Direct investigation of ' + asset_name + ' required'}
- Failure cascade: {'Will affect ' + str(len(downstream_nodes)) + ' downstream systems' if downstream_nodes else 'Isolated failure - no cascade'}
"""

        # Build detailed action log for Step 5
        step5_actions = []
        step5_actions.append(f"ðŸŽ¯ Synthesizing all data to identify root cause")
        step5_actions.append(f"ðŸ“Š Data sources analyzed:")
        step5_actions.append(f"   â”œâ”€ Node properties and status")
        step5_actions.append(f"   â”œâ”€ Log entries and error messages")
        step5_actions.append(f"   â”œâ”€ Upstream dependency failures ({len(upstream_nodes)} found)")
        step5_actions.append(f"   â””â”€ Downstream cascade impact ({len(downstream_nodes)} affected)")

        if upstream_nodes:
            step5_actions.append(f"")
            step5_actions.append(f"âœ… ROOT CAUSE IDENTIFIED: {root_cause_asset}")
            step5_actions.append(f"ðŸ“Œ Reason: {root_cause_reason}")
            step5_actions.append(f"ðŸ”— Failure chain: {' â†’ '.join(upstream_nodes + [asset_name])}")
            step5_actions.append(f"ðŸ“ˆ Confidence level: HIGH (upstream failure detected)")
            step5_actions.append(f"ðŸ’¡ Analysis: Failure originated in {root_cause_asset} and cascaded to {asset_name}")
        else:
            step5_actions.append(f"")
            step5_actions.append(f"âœ… ROOT CAUSE IDENTIFIED: {asset_name}")
            step5_actions.append(f"ðŸ“Œ Reason: {root_cause_reason}")
            step5_actions.append(f"ðŸ“ˆ Confidence level: MEDIUM (no upstream failures - isolated incident)")
            step5_actions.append(f"ðŸ’¡ Analysis: {asset_name} appears to be the origin point of the failure")

        trace_steps.append({
            "step": 5,
            "title": "Root Cause Identification",
            "description": f"ðŸ” Root Cause Analysis:\n" + "\n".join(step5_actions),
            "status": "completed",
            "thinking": thinking_step5.strip(),
            "data": {
                "rootCause": root_cause_asset,
                "reason": root_cause_reason,
                "confidence": "high" if upstream_nodes else "medium",
                "failureChain": upstream_nodes + [asset_name] if upstream_nodes else [asset_name]
            },
            "mcpTools": mcp_tools_step5,
            "nodesInvolved": [root_cause_asset],
            "findings": f"Root cause identified: {root_cause_asset} ({root_cause_reason})",
            "detailedActions": step5_actions,
            "timestamp": datetime.now().isoformat()
        })

        # Step 6: Generate Recommendations
        recommendations = []
        if upstream_nodes:
            recommendations.append(f"Repair {root_cause_asset} to restore {asset_name}")
        if downstream_nodes:
            recommendations.append(f"Monitor {len(downstream_nodes)} affected downstream systems")
        if not upstream_nodes:
            recommendations.append(f"Investigate {asset_name} directly - appears to be isolated failure")

        mcp_tools_step6 = [
            {"tool": "recommendation_engine", "action": "Generating remediation playbook", "timestamp": datetime.now().isoformat()},
            {"tool": "priority_calculator", "action": "Calculating priority based on impact", "timestamp": datetime.now().isoformat()},
            {"tool": "knowledge_base", "action": "Searching for similar past incidents", "timestamp": datetime.now().isoformat()}
        ]

        # Generate thinking block for Step 6
        priority_level = "critical" if len(downstream_nodes) > 3 else "high" if downstream_nodes else "medium"
        thinking_step6 = f"""
Remediation Strategy & Recommendations (GraphRAG Reasoning):

Question: What actions should be taken to resolve this incident?
- Inputs: Root cause ({root_cause_asset}), Impact ({len(downstream_nodes)} downstream), Priority
- Goal: Minimize downtime, prevent cascading failures, restore service
- Constraints: Resource availability, SLA requirements, safety protocols

Priority Assessment:
- Downstream systems affected: {len(downstream_nodes)}
- Upstream dependencies: {len(upstream_nodes)}
- Priority level: {priority_level.upper()}
- Business impact: {'CRITICAL - Multiple production systems down' if len(downstream_nodes) > 3 else 'HIGH - Service degradation' if downstream_nodes else 'MEDIUM - Isolated issue'}

Remediation Strategy:
{'1. IMMEDIATE: Repair root cause at ' + root_cause_asset if upstream_nodes else '1. IMMEDIATE: Investigate and repair ' + asset_name}
{'2. MONITOR: Watch ' + str(len(downstream_nodes)) + ' downstream systems for cascading failures' if downstream_nodes else '2. VERIFY: Confirm isolated failure - no cascade risk'}
3. PREVENT: Implement monitoring to catch similar failures earlier
4. DOCUMENT: Update runbooks with this incident pattern

Action Plan:
{chr(10).join('- ' + rec for rec in recommendations)}

Risk Mitigation:
{'- HIGH RISK: Failure cascade in progress - expedite repairs' if len(downstream_nodes) > 3 else '- MODERATE RISK: Contained failure - normal priority' if downstream_nodes else '- LOW RISK: Isolated failure - standard remediation'}
- Estimated recovery: {'URGENT - Requires immediate attention' if priority_level == 'critical' else 'Normal SLA timeframe'}
- Next steps: Execute remediation plan, monitor for resolution
"""

        # Build detailed action log for Step 6
        step6_actions = []
        step6_actions.append(f"ðŸ’¼ Generating remediation playbook based on analysis")
        step6_actions.append(f"ðŸ“Š Incident Summary:")
        step6_actions.append(f"   â”œâ”€ Root Cause: {root_cause_asset}")
        step6_actions.append(f"   â”œâ”€ Affected Asset: {asset_name}")
        step6_actions.append(f"   â”œâ”€ Upstream Failures: {len(upstream_nodes)}")
        step6_actions.append(f"   â”œâ”€ Downstream Impact: {len(downstream_nodes)} systems")
        step6_actions.append(f"   â””â”€ Priority: {priority_level.upper()}")

        step6_actions.append(f"")
        step6_actions.append(f"ðŸŽ¯ Recommended Actions:")
        for i, rec in enumerate(recommendations, 1):
            step6_actions.append(f"   {i}. {rec}")

        step6_actions.append(f"")
        step6_actions.append(f"â±ï¸ Timeline:")
        if priority_level == 'critical':
            step6_actions.append(f"   â””â”€ URGENT: Begin remediation immediately - production systems failing")
        elif priority_level == 'high':
            step6_actions.append(f"   â””â”€ HIGH PRIORITY: Address within current shift")
        else:
            step6_actions.append(f"   â””â”€ NORMAL: Follow standard SLA timeframes")

        step6_actions.append(f"")
        step6_actions.append(f"ðŸ“ž Notifications sent to:")
        step6_actions.append(f"   â”œâ”€ Operations Team (for immediate action)")
        step6_actions.append(f"   â”œâ”€ Maintenance Team (for {root_cause_asset} repair)")
        if len(downstream_nodes) > 0:
            step6_actions.append(f"   â””â”€ Production Managers (downstream impact alert)")

        trace_steps.append({
            "step": 6,
            "title": "Recommendations & Action Plan",
            "description": f"ðŸ“‹ Remediation Strategy:\n" + "\n".join(step6_actions),
            "status": "completed",
            "thinking": thinking_step6.strip(),
            "data": {
                "recommendations": recommendations,
                "priority": "critical" if len(downstream_nodes) > 3 else "high" if downstream_nodes else "medium"
            },
            "mcpTools": mcp_tools_step6,
            "findings": f"Generated {len(recommendations)} recommendations",
            "detailedActions": step6_actions,
            "timestamp": datetime.now().isoformat()
        })

        return {
            "incidentId": incident_id,
            "assetName": asset_name,
            "traceCompleted": True,
            "totalSteps": len(trace_steps),
            "steps": trace_steps,
            "summary": {
                "rootCause": root_cause_asset,
                "upstreamFailures": len(upstream_nodes),
                "downstreamImpact": len(downstream_nodes),
                "totalNodesAnalyzed": len(set(upstream_nodes + [asset_name] + downstream_nodes)),
                "recommendations": recommendations
            }
        }


@app.get("/api/rca/related-incidents/{asset_name}")
def get_related_incidents(asset_name: str, time_range_hours: int = 24):
    """
    Get all incidents related to an asset and its connected nodes
    Returns incidents from the asset and its upstream/downstream connections
    """
    with driver.session() as session:
        result = session.run("""
            // Find the target asset and related assets
            MATCH (asset:Asset {name: $assetName})
            OPTIONAL MATCH path = (asset)-[:CONNECTS_TO|POWERS|FEEDS_DATA|DEPENDS_ON*1..2]-(related:Asset)

            WITH asset, collect(DISTINCT related) as relatedAssets

            // Get all assets with failures or issues
            UNWIND [asset] + relatedAssets as a
            WITH a
            WHERE a.status IN ['offline', 'error', 'warning', 'degraded', 'unreachable', 'failed']

            WITH a, a.lastFailure as failureTime
            ORDER BY failureTime DESC

            RETURN {
                incidentId: 'INC-' + toString(id(a)),
                assetName: a.name,
                assetType: a.type,
                status: a.status,
                failureReason: coalesce(a.failureReason, a.issue, 'Unknown'),
                failureTime: toString(a.lastFailure),
                severity: CASE
                    WHEN a.status IN ['offline', 'error', 'failed'] THEN 'critical'
                    WHEN a.status IN ['unreachable', 'degraded'] THEN 'high'
                    WHEN a.status = 'warning' THEN 'medium'
                    ELSE 'low'
                END,
                relatedTo: $assetName,
                ipAddress: a.ipAddress
            } as incident
            LIMIT 20
        """, assetName=asset_name)

        incidents = [dict(record["incident"]) for record in result]

        return {
            "assetName": asset_name,
            "totalIncidents": len(incidents),
            "timeRangeHours": time_range_hours,
            "incidents": incidents
        }


@app.post("/api/setup/graph-relationships")
def setup_graph_relationships():
    """
    Set up dependency relationships in the graph for realistic factory topology
    Creates POWERS, CONNECTS_TO, FEEDS_DATA, CONTROLS relationships
    """
    with driver.session() as session:
        # Create power distribution topology: UPS -> PLCs, Switches, Gateways
        session.run("""
            MATCH (ups:Asset {name: 'UPS-Main'})
            MATCH (plc1:Asset {name: 'PLC-001'})
            MATCH (plc2:Asset {name: 'PLC-002'})
            MATCH (plc3:Asset {name: 'PLC-003'})
            MATCH (plc5:Asset {name: 'PLC-005'})
            MATCH (core:Asset {name: 'CoreSwitch-Datacenter'})
            MATCH (switch05:Asset {name: 'NetworkSwitch-05'})

            MERGE (ups)-[:POWERS]->(plc1)
            MERGE (ups)-[:POWERS]->(plc2)
            MERGE (ups)-[:POWERS]->(plc3)
            MERGE (ups)-[:POWERS]->(plc5)
            MERGE (ups)-[:POWERS]->(core)
            MERGE (ups)-[:POWERS]->(switch05)
        """)

        # Create network topology: Core Switch -> Edge Switches -> Gateways
        session.run("""
            MATCH (core:Asset {name: 'CoreSwitch-Datacenter'})
            MATCH (switch01:Asset {name: 'NetworkSwitch-01'})
            MATCH (switch05:Asset {name: 'NetworkSwitch-05'})
            MATCH (gateway01:Asset {name: 'EdgeGateway-01'})
            MATCH (gateway02:Asset {name: 'EdgeGateway-02'})

            MERGE (core)-[:CONNECTS_TO]->(switch01)
            MERGE (core)-[:CONNECTS_TO]->(switch05)
            MERGE (switch01)-[:CONNECTS_TO]->(gateway01)
            MERGE (switch05)-[:CONNECTS_TO]->(gateway02)
        """)

        # Create data flow: Gateways -> Robots, PLCs -> Conveyors/Robots
        session.run("""
            MATCH (gateway01:Asset {name: 'EdgeGateway-01'})
            MATCH (gateway02:Asset {name: 'EdgeGateway-02'})
            MATCH (robot01:Asset {name: 'Robot-01'})
            MATCH (robot02:Asset {name: 'Robot-02'})
            MATCH (robot03:Asset {name: 'Robot-03'})
            MATCH (robot04:Asset {name: 'Robot-04'})

            MERGE (gateway01)-[:FEEDS_DATA]->(robot01)
            MERGE (gateway01)-[:FEEDS_DATA]->(robot02)
            MERGE (gateway02)-[:FEEDS_DATA]->(robot03)
            MERGE (gateway02)-[:FEEDS_DATA]->(robot04)
        """)

        # Create control relationships: PLCs -> Equipment
        session.run("""
            MATCH (plc1:Asset {name: 'PLC-001'})
            MATCH (plc2:Asset {name: 'PLC-002'})
            MATCH (conveyor:Asset {name: 'Conveyor-01'})
            MATCH (robot01:Asset {name: 'Robot-01'})
            MATCH (robot02:Asset {name: 'Robot-02'})

            MERGE (plc1)-[:CONTROLS]->(conveyor)
            MERGE (plc2)-[:CONTROLS]->(robot01)
            MERGE (plc2)-[:CONTROLS]->(robot02)
        """)

        # Create sensor data feeds: Sensors -> PLCs/Gateways
        session.run("""
            MATCH (sensor:Asset {name: 'Sensor-001'})
            MATCH (switch05:Asset {name: 'NetworkSwitch-05'})
            MATCH (plc1:Asset {name: 'PLC-001'})

            MERGE (switch05)-[:CONNECTS_TO]->(sensor)
            MERGE (sensor)-[:FEEDS_DATA]->(plc1)
        """)

        # Count relationships created
        result = session.run("""
            MATCH ()-[r:POWERS|CONNECTS_TO|FEEDS_DATA|CONTROLS]->()
            RETURN count(r) as totalRelationships,
                   count(CASE WHEN type(r) = 'POWERS' THEN 1 END) as powers,
                   count(CASE WHEN type(r) = 'CONNECTS_TO' THEN 1 END) as connects,
                   count(CASE WHEN type(r) = 'FEEDS_DATA' THEN 1 END) as feeds,
                   count(CASE WHEN type(r) = 'CONTROLS' THEN 1 END) as controls
        """)

        stats = result.single()

        return {
            "status": "success",
            "message": "Graph relationships created successfully",
            "relationships": {
                "total": stats["totalRelationships"],
                "POWERS": stats["powers"],
                "CONNECTS_TO": stats["connects"],
                "FEEDS_DATA": stats["feeds"],
                "CONTROLS": stats["controls"]
            },
            "topology": {
                "power": "UPS-Main powers PLCs and network switches",
                "network": "CoreSwitch connects to edge switches, which connect to gateways",
                "dataFlow": "Gateways feed data to robots, sensors feed to PLCs",
                "control": "PLCs control conveyors and robots"
            }
        }


@app.post("/api/setup/cascading-failures")
def setup_cascading_failures():
    """
    Set up interconnected failure scenarios for demo purposes
    Creates realistic cascading failure chains with proper downstream impacts
    """
    with driver.session() as session:
        # First, reset some assets to online to create a clean state
        session.run("""
            MATCH (a:Asset)
            WHERE a.name IN ['UPS-Main', 'PLC-001', 'PLC-002', 'PLC-003', 'PLC-005',
                             'CoreSwitch-Datacenter', 'NetworkSwitch-01', 'NetworkSwitch-05',
                             'EdgeGateway-01', 'EdgeGateway-02', 'Robot-01', 'Robot-02',
                             'Robot-03', 'Robot-04', 'Conveyor-01', 'Sensor-001']
            SET a.status = 'online',
                a.failureReason = null
        """)

        # Scenario 1: UPS-Main Failure causing CASCADE of downstream PLC and equipment failures
        result1 = session.run("""
            MATCH (ups:Asset {name: 'UPS-Main'})
            SET ups.status = 'offline',
                ups.failureReason = 'Main power transformer failure - circuit breaker tripped',
                ups.lastFailure = datetime()
            WITH ups

            // Find all assets powered by UPS (direct and 2-hop downstream)
            MATCH path = (ups)-[:POWERS*1..2]->(downstream:Asset)
            SET downstream.status = 'offline',
                downstream.failureReason = 'Power loss - upstream UPS-Main failure',
                downstream.lastFailure = datetime()

            WITH ups, collect(DISTINCT downstream.name) as affected
            RETURN ups.name as source, affected, size(affected) as affectedCount
        """)

        scenario1 = result1.single()

        # Scenario 2: Core Network Switch failure affecting edge devices and creating degraded state
        result2 = session.run("""
            MATCH (switch:Asset {name: 'CoreSwitch-Datacenter'})
            SET switch.status = 'error',
                switch.failureReason = 'Network loop detected - high packet loss (95%+)',
                switch.lastFailure = datetime()
            WITH switch

            // All directly connected devices go to error state
            MATCH (switch)-[:CONNECTS_TO]->(edge:Asset)
            SET edge.status = 'error',
                edge.failureReason = 'Network connectivity degraded via CoreSwitch-Datacenter',
                edge.lastFailure = datetime()

            WITH switch, collect(DISTINCT edge.name) as affected
            RETURN switch.name as source, affected, size(affected) as affectedCount
        """)

        scenario2 = result2.single()

        # Scenario 3: Gateway failure impacting multiple robots via data feed loss
        result3 = session.run("""
            MATCH (gateway:Asset {name: 'EdgeGateway-02'})
            SET gateway.status = 'offline',
                gateway.failureReason = 'MQTT broker connection timeout - SSL certificate expired',
                gateway.lastFailure = datetime()
            WITH gateway

            // All robots fed by this gateway lose data connectivity
            MATCH (gateway)-[:FEEDS_DATA|CONTROLS]->(robot:Asset)
            SET robot.status = 'error',
                robot.failureReason = 'Data feed lost from ' + gateway.name,
                robot.lastFailure = datetime()

            WITH gateway, collect(DISTINCT robot.name) as affected
            RETURN gateway.name as source, affected, size(affected) as affectedCount
        """)

        scenario3 = result3.single()

        # Scenario 4: Create a multi-hop cascade - NetworkSwitch affecting sensors affecting production
        result4 = session.run("""
            MATCH (switch:Asset {name: 'NetworkSwitch-05'})
            SET switch.status = 'offline',
                switch.failureReason = 'Hardware failure - power supply unit overheated',
                switch.lastFailure = datetime()
            WITH switch

            // Multi-hop cascade: switch -> sensors -> equipment
            MATCH path = (switch)-[:CONNECTS_TO|FEEDS_DATA*1..3]->(downstream:Asset)
            WHERE downstream.type IN ['Sensor', 'PLC', 'IndustrialRobot', 'Conveyor']
            SET downstream.status = CASE
                WHEN length(path) = 1 THEN 'offline'
                WHEN length(path) = 2 THEN 'error'
                ELSE 'degraded'
            END,
            downstream.failureReason = 'Cascading failure from NetworkSwitch-05 (' +
                                       toString(length(path)) + ' hops away)',
            downstream.lastFailure = datetime()

            WITH switch, collect(DISTINCT downstream.name) as affected
            RETURN switch.name as source, affected, size(affected) as affectedCount
        """)

        scenario4 = result4.single()

        return {
            "status": "success",
            "message": "Enhanced cascading failure scenarios created with realistic downstream impacts",
            "scenarios": [
                {
                    "name": "UPS Power Cascade",
                    "source": scenario1["source"] if scenario1 else "UPS-Main",
                    "affected": scenario1["affected"] if scenario1 else [],
                    "count": scenario1["affectedCount"] if scenario1 else 0,
                    "description": "UPS transformer failure causing power loss to all downstream equipment"
                },
                {
                    "name": "Network Degradation Cascade",
                    "source": scenario2["source"] if scenario2 else "CoreSwitch-Datacenter",
                    "affected": scenario2["affected"] if scenario2 else [],
                    "count": scenario2["affectedCount"] if scenario2 else 0,
                    "description": "Core switch network loop causing connectivity issues"
                },
                {
                    "name": "Data Feed Cascade",
                    "source": scenario3["source"] if scenario3 else "EdgeGateway-02",
                    "affected": scenario3["affected"] if scenario3 else [],
                    "count": scenario3["affectedCount"] if scenario3 else 0,
                    "description": "Gateway MQTT failure cutting data feeds to production robots"
                },
                {
                    "name": "Multi-Hop Network Cascade",
                    "source": scenario4["source"] if scenario4 else "NetworkSwitch-05",
                    "affected": scenario4["affected"] if scenario4 else [],
                    "count": scenario4["affectedCount"] if scenario4 else 0,
                    "description": "Network switch hardware failure propagating through sensors to equipment"
                }
            ],
            "testAssets": {
                "UPS_CASCADE": "UPS-Main",
                "NETWORK_CASCADE": "CoreSwitch-Datacenter",
                "DATA_FEED_CASCADE": "EdgeGateway-02",
                "MULTI_HOP_CASCADE": "NetworkSwitch-05"
            },
            "totalAffectedAssets": (
                (scenario1["affectedCount"] if scenario1 else 0) +
                (scenario2["affectedCount"] if scenario2 else 0) +
                (scenario3["affectedCount"] if scenario3 else 0) +
                (scenario4["affectedCount"] if scenario4 else 0)
            )
        }


@app.get("/api/graph/dependencies")
def get_all_dependencies():
    """Quick Query: Show all asset dependencies"""
    with driver.session() as session:
        result = session.run("""
            MATCH (source:Asset)-[r:POWERS|CONNECTS_TO|FEEDS_DATA|CONTROLS]->(target:Asset)
            RETURN {
                source: source.name,
                sourceType: source.type,
                relationship: type(r),
                target: target.name,
                targetType: target.type,
                sourceStatus: source.status,
                targetStatus: target.status
            } as dependency
            LIMIT 100
        """)

        dependencies = [dict(record["dependency"]) for record in result]
        return {
            "totalDependencies": len(dependencies),
            "dependencies": dependencies,
            "cypher": "MATCH (source:Asset)-[r:POWERS|CONNECTS_TO|FEEDS_DATA|CONTROLS]->(target:Asset) RETURN source, r, target"
        }


@app.get("/api/graph/critical-paths")
def get_critical_paths():
    """Quick Query: Find assets with high downstream impact"""
    with driver.session() as session:
        result = session.run("""
            MATCH (asset:Asset)
            OPTIONAL MATCH (asset)-[:POWERS|CONNECTS_TO|FEEDS_DATA|CONTROLS*1..3]->(downstream:Asset)
            WITH asset, count(DISTINCT downstream) as downstreamCount
            WHERE downstreamCount > 0
            RETURN {
                asset: asset.name,
                type: asset.type,
                status: asset.status,
                downstreamCount: downstreamCount,
                criticality: CASE
                    WHEN downstreamCount > 10 THEN 'CRITICAL'
                    WHEN downstreamCount > 5 THEN 'HIGH'
                    WHEN downstreamCount > 2 THEN 'MEDIUM'
                    ELSE 'LOW'
                END
            } as criticalAsset
            ORDER BY downstreamCount DESC
            LIMIT 20
        """)

        paths = [dict(record["criticalAsset"]) for record in result]
        return {
            "totalCriticalAssets": len(paths),
            "criticalAssets": paths,
            "insight": f"Found {len(paths)} assets with downstream dependencies"
        }


@app.get("/api/rca/failure-cascades")
def get_failure_cascades():
    """Quick Query: Show current failure cascades"""
    with driver.session() as session:
        result = session.run("""
            MATCH (failed:Asset)
            WHERE failed.status IN ['offline', 'error']
            CALL {
                WITH failed
                OPTIONAL MATCH path = (failed)-[:POWERS|CONNECTS_TO|FEEDS_DATA|CONTROLS*1..3]->(affected:Asset)
                WHERE affected.status IN ['offline', 'error']
                RETURN collect(DISTINCT {
                    name: affected.name,
                    type: affected.type,
                    status: affected.status,
                    distance: length(path)
                }) as affectedAssets
            }
            WITH failed,
                 [item in affectedAssets WHERE item.name IS NOT NULL] as validAffected
            WHERE size(validAffected) > 0
            RETURN {
                sourceFailure: failed.name,
                sourceType: failed.type,
                sourceReason: failed.failureReason,
                cascadeSize: size(validAffected),
                affectedAssets: validAffected
            } as cascade
            ORDER BY size(validAffected) DESC
        """)

        cascades = [dict(record["cascade"]) for record in result]
        return {
            "totalCascades": len(cascades),
            "cascades": cascades,
            "insight": f"Found {len(cascades)} active failure cascades in the system"
        }


@app.get("/api/rca/upstream-analysis-all")
def get_upstream_analysis_all():
    """Quick Query: Upstream dependency analysis for all failing assets"""
    with driver.session() as session:
        result = session.run("""
            MATCH (failed:Asset)
            WHERE failed.status IN ['offline', 'error', 'degraded']
            CALL {
                WITH failed
                OPTIONAL MATCH path = (upstream:Asset)-[:POWERS|CONNECTS_TO|FEEDS_DATA|CONTROLS*1..3]->(failed)
                RETURN collect(DISTINCT {
                    name: upstream.name,
                    type: upstream.type,
                    status: upstream.status,
                    distance: length(path),
                    ipAddress: upstream.ipAddress,
                    failureReason: upstream.failureReason
                }) as upstreamDeps
            }
            WITH failed,
                 [item in upstreamDeps WHERE item.name IS NOT NULL] as validUpstream
            WHERE size(validUpstream) > 0
            RETURN {
                asset: failed.name,
                assetType: failed.type,
                status: failed.status,
                failureReason: failed.failureReason,
                upstreamCount: size(validUpstream),
                upstreamAssets: validUpstream
            } as analysis
            ORDER BY size(validUpstream) DESC
        """)

        analyses = [dict(record["analysis"]) for record in result]
        return {
            "totalFailingAssets": len(analyses),
            "analyses": analyses,
            "insight": f"Analyzed upstream dependencies for {len(analyses)} failing assets. This helps identify potential root causes."
        }


@app.get("/api/rca/blast-radius-all")
def get_blast_radius_all():
    """Quick Query: Calculate blast radius for all critical assets"""
    with driver.session() as session:
        result = session.run("""
            MATCH (critical:Asset)
            WHERE critical.type IN ['UPS', 'PowerDistribution', 'NetworkSwitch', 'EdgeGateway', 'PLCController']
            CALL {
                WITH critical
                OPTIONAL MATCH path = (critical)-[:POWERS|CONNECTS_TO|FEEDS_DATA|DEPENDS_ON|CONTROLS*1..4]->(affected:Asset)
                RETURN collect(DISTINCT {
                    name: affected.name,
                    type: affected.type,
                    status: affected.status,
                    distance: length(path)
                }) as potentialImpact
            }
            WITH critical,
                 [item in potentialImpact WHERE item.name IS NOT NULL] as validImpact
            WHERE size(validImpact) > 0
            RETURN {
                criticalAsset: critical.name,
                assetType: critical.type,
                currentStatus: critical.status,
                blastRadius: size(validImpact),
                affectedSystems: validImpact,
                riskLevel: CASE
                    WHEN size(validImpact) >= 10 THEN 'CRITICAL'
                    WHEN size(validImpact) >= 5 THEN 'HIGH'
                    WHEN size(validImpact) >= 2 THEN 'MEDIUM'
                    ELSE 'LOW'
                END
            } as radius
            ORDER BY size(validImpact) DESC
        """)

        radii = [dict(record["radius"]) for record in result]
        return {
            "totalCriticalAssets": len(radii),
            "blastRadii": radii,
            "insight": f"Calculated blast radius for {len(radii)} critical infrastructure assets. This shows potential impact if these assets fail."
        }


@app.get("/api/spaces")
def get_spaces():
    """Get spaces with Matterport links"""
    with driver.session() as session:
        result = session.run("""
            MATCH (space:Space)
            OPTIONAL MATCH (space)<-[:LOCATED_IN]-(asset:Asset)
            WITH space, count(DISTINCT asset) as assetCount
            RETURN {
                id: space.id,
                name: space.name,
                level: space.level,
                matterportUrl: space.matterportUrl,
                hasVirtualTour: coalesce(space.hasVirtualTour, false),
                assetCount: assetCount
            } as space
            ORDER BY space.level
        """)

        return [dict(record["space"]) for record in result]


# ============================================================================
# GitOps Configuration & Drift Detection Endpoints
# ============================================================================

def get_gitops_config_for_asset(asset_name: str, asset_type: str) -> dict:
    """
    Simulate fetching GitOps configuration from Git repository
    In production, this would fetch from GitHub API or local Git repo
    """
    # GitOps intended configuration (what SHOULD be)
    gitops_configs = {
        "PLC-001": {
            "name": "PLC-001",
            "type": "PLC",
            "status": "running",
            "ipAddress": "192.168.1.10",
            "version": "2.5.0",
            "configChecksum": "a1b2c3d4",
            "securityZone": "Level 1 - Control",
            "location": "Assembly Line 1",
            "port": 502,
            "scanRate": 100,
            "gitRepo": "github.com/factory/plc-configs",
            "gitPath": "plcs/assembly-line-1/plc-001.yaml",
            "lastCommit": "abc123",
            "lastUpdated": "2026-01-05T10:30:00Z"
        },
        "NetworkSwitch-01": {
            "name": "NetworkSwitch-01",
            "type": "NetworkSwitch",
            "status": "running",
            "ipAddress": "192.168.0.5",
            "version": "16.9.3",
            "configChecksum": "e5f6g7h8",
            "securityZone": "Level 2 - Supervisory",
            "location": "Network Closet A",
            "vlanConfig": "10,20,30,40",
            "ports": 48,
            "gitRepo": "github.com/factory/network-configs",
            "gitPath": "switches/core/switch-01.yaml",
            "lastCommit": "def456",
            "lastUpdated": "2026-01-04T15:20:00Z"
        },
        "Robot-Arm-101": {
            "name": "Robot-Arm-101",
            "type": "IndustrialRobot",
            "status": "running",
            "ipAddress": "192.168.2.15",
            "version": "7.2.1",
            "configChecksum": "i9j0k1l2",
            "securityZone": "Level 1 - Control",
            "location": "Assembly Line 2",
            "maxPayload": 50,
            "reach": 1800,
            "gitRepo": "github.com/factory/robot-configs",
            "gitPath": "robots/assembly/robot-arm-101.yaml",
            "lastCommit": "ghi789",
            "lastUpdated": "2026-01-03T09:15:00Z"
        },
        "SCADA-HMI-01": {
            "name": "SCADA-HMI-01",
            "type": "HMI",
            "status": "running",
            "ipAddress": "192.168.3.20",
            "version": "12.4.0",
            "configChecksum": "m3n4o5p6",
            "securityZone": "Level 2 - Supervisory",
            "location": "Control Room",
            "screens": 3,
            "resolution": "1920x1080",
            "gitRepo": "github.com/factory/scada-configs",
            "gitPath": "hmi/control-room/scada-hmi-01.yaml",
            "lastCommit": "jkl012",
            "lastUpdated": "2026-01-02T14:00:00Z"
        }
    }

    # Return config for specific asset or generate default
    if asset_name in gitops_configs:
        return gitops_configs[asset_name]

    # Generate default config for assets not in the predefined list
    return {
        "name": asset_name,
        "type": asset_type,
        "status": "running",
        "ipAddress": f"192.168.{random.randint(1,5)}.{random.randint(10,250)}",
        "version": f"{random.randint(1,10)}.{random.randint(0,9)}.{random.randint(0,9)}",
        "configChecksum": f"{random.randint(1000,9999):04x}",
        "securityZone": "Level 1 - Control",
        "gitRepo": f"github.com/factory/{asset_type.lower()}-configs",
        "gitPath": f"{asset_type.lower()}s/{asset_name.lower()}.yaml",
        "lastCommit": f"commit{random.randint(100,999)}",
        "lastUpdated": (datetime.now() - timedelta(days=random.randint(1,30))).isoformat()
    }


@app.get("/api/gitops/config")
def get_all_gitops_configs():
    """
    Get GitOps intended configuration for all assets from Git repository
    This represents the INTENDED state (what should be deployed)
    """
    with driver.session() as session:
        result = session.run("""
            MATCH (asset:Asset)
            RETURN asset.name as name, asset.type as type
            ORDER BY asset.name
            LIMIT 50
        """)

        configs = []
        for record in result:
            asset_name = record["name"]
            asset_type = record["type"]
            config = get_gitops_config_for_asset(asset_name, asset_type)
            configs.append(config)

        return {
            "totalAssets": len(configs),
            "repository": "github.com/factory-org/factory-digital-twin-gitops",
            "branch": "main",
            "lastSync": datetime.now().isoformat(),
            "configs": configs
        }


@app.get("/api/gitops/actual")
def get_actual_state():
    """
    Get ACTUAL observed state from discovery agents
    This represents what is currently running in the factory
    """
    with driver.session() as session:
        result = session.run("""
            MATCH (asset:Asset)
            RETURN {
                name: asset.name,
                type: asset.type,
                status: asset.status,
                ipAddress: asset.ipAddress,
                version: asset.version,
                configChecksum: coalesce(asset.configChecksum, 'unknown'),
                securityZone: asset.securityZone,
                location: coalesce(asset.location, 'Unknown'),
                lastSeen: toString(coalesce(asset.lastSeen, datetime())),
                discoveryAgent: coalesce(asset.discoveryAgent, 'network-scanner')
            } as actualState
            ORDER BY asset.name
            LIMIT 50
        """)

        actual_states = [dict(record["actualState"]) for record in result]

        return {
            "totalAssets": len(actual_states),
            "discoveryTime": datetime.now().isoformat(),
            "discoveryMethod": "Multi-agent discovery (Network Scanner, SNMP, Modbus, OPC-UA)",
            "actualStates": actual_states
        }


@app.get("/api/gitops/drift")
def calculate_drift():
    """
    Calculate drift between GitOps intended config and actual observed state
    Returns detailed drift analysis for each asset
    """
    with driver.session() as session:
        result = session.run("""
            MATCH (asset:Asset)
            RETURN {
                name: asset.name,
                type: asset.type,
                status: asset.status,
                ipAddress: asset.ipAddress,
                version: asset.version,
                configChecksum: coalesce(asset.configChecksum, 'unknown'),
                securityZone: asset.securityZone,
                location: coalesce(asset.location, 'Unknown')
            } as actualState
            ORDER BY asset.name
            LIMIT 50
        """)

        drift_records = []
        total_drifted = 0
        critical_drift = 0

        for record in result:
            actual = dict(record["actualState"])
            intended = get_gitops_config_for_asset(actual["name"], actual["type"])

            # Detect drift in each field
            drifts = []
            drift_severity = "none"

            # Status drift
            if actual.get("status", "").lower() != intended.get("status", "").lower():
                drifts.append({
                    "field": "status",
                    "intended": intended.get("status"),
                    "actual": actual.get("status"),
                    "severity": "critical" if actual.get("status") in ["offline", "error", "failed"] else "high"
                })
                drift_severity = "critical"
                critical_drift += 1

            # IP Address drift
            if actual.get("ipAddress") != intended.get("ipAddress"):
                drifts.append({
                    "field": "ipAddress",
                    "intended": intended.get("ipAddress"),
                    "actual": actual.get("ipAddress"),
                    "severity": "medium"
                })
                if drift_severity == "none":
                    drift_severity = "medium"

            # Version drift
            if actual.get("version") != intended.get("version"):
                drifts.append({
                    "field": "version",
                    "intended": intended.get("version"),
                    "actual": actual.get("version"),
                    "severity": "high"
                })
                if drift_severity in ["none", "medium"]:
                    drift_severity = "high"

            # Config checksum drift
            if actual.get("configChecksum") != intended.get("configChecksum"):
                if actual.get("configChecksum") != "unknown":
                    drifts.append({
                        "field": "configChecksum",
                        "intended": intended.get("configChecksum"),
                        "actual": actual.get("configChecksum"),
                        "severity": "high"
                    })
                    if drift_severity in ["none", "medium"]:
                        drift_severity = "high"

            # Security zone drift
            if actual.get("securityZone") != intended.get("securityZone"):
                drifts.append({
                    "field": "securityZone",
                    "intended": intended.get("securityZone"),
                    "actual": actual.get("securityZone"),
                    "severity": "critical"
                })
                drift_severity = "critical"
                critical_drift += 1

            if drifts:
                total_drifted += 1
                drift_records.append({
                    "assetName": actual["name"],
                    "assetType": actual["type"],
                    "driftStatus": drift_severity,
                    "driftCount": len(drifts),
                    "drifts": drifts,
                    "gitRepo": intended.get("gitRepo"),
                    "gitPath": intended.get("gitPath"),
                    "lastCommit": intended.get("lastCommit"),
                    "detectedAt": datetime.now().isoformat(),
                    "actions": generate_drift_actions(drifts, actual["name"])
                })

        return {
            "summary": {
                "totalAssets": len(drift_records) + (50 - total_drifted),
                "driftedAssets": total_drifted,
                "inSyncAssets": 50 - total_drifted,
                "criticalDrifts": critical_drift,
                "driftPercentage": round((total_drifted / 50) * 100, 1)
            },
            "drifts": drift_records,
            "lastCalculated": datetime.now().isoformat()
        }


def generate_drift_actions(drifts: list, asset_name: str) -> list:
    """Generate recommended actions for resolving drift"""
    actions = []

    for drift in drifts:
        field = drift["field"]
        severity = drift["severity"]

        if field == "status":
            actions.append({
                "action": "investigate_failure",
                "title": "Investigate Asset Failure",
                "description": f"Asset status drifted to {drift['actual']}. Run RCA to identify root cause.",
                "priority": "critical",
                "automated": False
            })
        elif field == "version":
            actions.append({
                "action": "sync_version",
                "title": "Update to Intended Version",
                "description": f"Upgrade/downgrade from {drift['actual']} to {drift['intended']}",
                "priority": "high",
                "automated": True
            })
        elif field == "ipAddress":
            actions.append({
                "action": "update_network",
                "title": "Update Network Configuration",
                "description": f"Reconfigure IP from {drift['actual']} to {drift['intended']}",
                "priority": "medium",
                "automated": True
            })
        elif field == "configChecksum":
            actions.append({
                "action": "sync_config",
                "title": "Sync Configuration from Git",
                "description": "Configuration has drifted. Pull latest config from GitOps repository.",
                "priority": "high",
                "automated": True
            })
        elif field == "securityZone":
            actions.append({
                "action": "update_zone",
                "title": "Reassign Security Zone",
                "description": f"Move asset from {drift['actual']} to {drift['intended']}",
                "priority": "critical",
                "automated": False
            })

    return actions


@app.get("/api/gitops/drift/history")
def get_drift_history(days: int = 7):
    """
    Get drift history over time
    Shows trend of drift detection over the past N days
    """
    history = []

    for day in range(days, -1, -1):
        date = datetime.now() - timedelta(days=day)

        # Simulate historical drift data
        # In production, this would query a drift_history table
        total_assets = 50
        drifted = random.randint(5, 15) if day > 0 else 12
        critical = random.randint(1, 5) if day > 0 else 3

        history.append({
            "date": date.strftime("%Y-%m-%d"),
            "timestamp": date.isoformat(),
            "totalAssets": total_assets,
            "driftedAssets": drifted,
            "criticalDrifts": critical,
            "driftPercentage": round((drifted / total_assets) * 100, 1)
        })

    return {
        "timeRange": f"Last {days} days",
        "history": history
    }


@app.post("/api/gitops/drift/resolve")
def resolve_drift(request: dict):
    """
    Resolve drift by syncing actual state to match GitOps config
    Actions: sync_config, update_network, sync_version, ignore, update_git
    """
    asset_name = request.get("assetName")
    action = request.get("action")
    field = request.get("field")

    # Simulate drift resolution
    resolution_messages = {
        "sync_config": f"Configuration synced from Git for {asset_name}. Checksum now matches GitOps state.",
        "update_network": f"Network configuration updated for {asset_name}. IP address synchronized.",
        "sync_version": f"Version update initiated for {asset_name}. Will be deployed in next maintenance window.",
        "ignore": f"Drift ignored for {asset_name}. Marked as acceptable deviation.",
        "update_git": f"GitOps repository updated to match actual state of {asset_name}. New commit created."
    }

    return {
        "success": True,
        "assetName": asset_name,
        "action": action,
        "field": field,
        "message": resolution_messages.get(action, "Drift resolution completed"),
        "resolvedAt": datetime.now().isoformat(),
        "nextSync": (datetime.now() + timedelta(hours=1)).isoformat()
    }


@app.get("/api/gitops/drift/stats")
def get_drift_statistics():
    """
    Get comprehensive drift statistics and analytics
    """
    return {
        "overview": {
            "totalAssets": 50,
            "driftedAssets": 12,
            "criticalDrifts": 3,
            "highDrifts": 5,
            "mediumDrifts": 4,
            "syncedAssets": 38,
            "driftPercentage": 24.0
        },
        "byType": [
            {"type": "PLC", "total": 15, "drifted": 3, "driftRate": 20.0},
            {"type": "Robot", "total": 8, "drifted": 2, "driftRate": 25.0},
            {"type": "NetworkSwitch", "total": 10, "drifted": 3, "driftRate": 30.0},
            {"type": "HMI", "total": 5, "drifted": 1, "driftRate": 20.0},
            {"type": "Sensor", "total": 12, "drifted": 3, "driftRate": 25.0}
        ],
        "byField": [
            {"field": "status", "count": 3, "severity": "critical"},
            {"field": "version", "count": 5, "severity": "high"},
            {"field": "configChecksum", "count": 4, "severity": "high"},
            {"field": "ipAddress", "count": 2, "severity": "medium"},
            {"field": "securityZone", "count": 1, "severity": "critical"}
        ],
        "timeline": {
            "last24h": 2,
            "last7d": 8,
            "last30d": 12
        },
        "topDrifted": [
            {"asset": "PLC-001", "driftCount": 3, "severity": "critical"},
            {"asset": "NetworkSwitch-01", "driftCount": 2, "severity": "high"},
            {"asset": "Robot-Arm-101", "driftCount": 2, "severity": "high"}
        ]
    }


@app.on_event("shutdown")
def shutdown_event():
    driver.close()


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
